[{"categories":["Finance"],"contents":"Introduction The Seasonal Autoregressive Integrated Moving Average (SARIMA) model is an extension of the ARIMA model (discussed in the previous article) that incorporates seasonality. This makes it particularly useful for analyzing financial time series data, which often exhibits both trend and seasonal patterns. In this article, we\u0026rsquo;ll apply the SARIMA model to Apple (AAPL) stock data, perform signal decomposition, and provide a detailed mathematical explanation of the model.\n1. Data Preparation and Exploration First, let\u0026rsquo;s obtain the Apple stock data and prepare it for analysis:\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import yfinance as yf from statsmodels.tsa.seasonal import seasonal_decompose from statsmodels.tsa.statespace.sarimax import SARIMAX from statsmodels.tsa.stattools import adfuller, acf, pacf from pmdarima import auto_arima import seaborn as sns # Download Apple stock data ticker = \u0026#34;AAPL\u0026#34; start_date = \u0026#34;2022-01-01\u0026#34; end_date = \u0026#34;2024-01-23\u0026#34; data = yf.download(ticker, start=start_date, end=end_date) df = data.copy() # Use closing prices for our analysis ts = data[\u0026#39;Close\u0026#39;].dropna() # Plot the time series plt.figure(figsize=(12,6)) plt.plot(ts) plt.title(\u0026#39;Apple Stock Closing Prices\u0026#39;) plt.xlabel(\u0026#39;Date\u0026#39;) plt.ylabel(\u0026#39;Price\u0026#39;) plt.show() [*********************100%%**********************] 1 of 1 completed\r2. Signal Decomposition Before we build our SARIMA model, it\u0026rsquo;s crucial to understand the components of our time series. We\u0026rsquo;ll use seasonal decomposition to break down the series into trend, seasonal, and residual components:\n# Perform seasonal decomposition decomposition = seasonal_decompose(ts, model=\u0026#39;additive\u0026#39;, period=12) # 252 trading days in a year plt.rcParams.update({\u0026#39;figure.dpi\u0026#39;:200}) # Plot the decomposition fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 16)) decomposition.observed.plot(ax=ax1) ax1.set_title(\u0026#39;Observed\u0026#39;) decomposition.trend.plot(ax=ax2, c=\u0026#39;r\u0026#39;) ax2.set_title(\u0026#39;Trend\u0026#39;) decomposition.seasonal.plot(ax=ax3, c=\u0026#39;k\u0026#39;) ax3.set_title(\u0026#39;Seasonal\u0026#39;) decomposition.resid.plot(ax=ax4, c=\u0026#39;g\u0026#39;) ax4.set_title(\u0026#39;Residual\u0026#39;) plt.tight_layout() plt.show() This decomposition helps us understand:\nTrend: The long-term progression of the series Seasonality: Repeating patterns or cycles Residual: The noise left after accounting for trend and seasonality 3. Mathematical Formulation of SARIMA The SARIMA model is denoted as SARIMA(p,d,q)(P,D,Q)m, where:\np, d, q: Non-seasonal orders of AR, differencing, and MA P, D, Q: Seasonal orders of AR, differencing, and MA m: Number of periods per season The mathematical formulation of SARIMA combines the non-seasonal and seasonal components:\nNon-seasonal components:\n$AR(p): φ(B) = 1 - φ₁B - φ₂B² - \u0026hellip; - φₚBᵖ$ $MA(q): θ(B) = 1 + θ₁B + θ₂B² + \u0026hellip; + θ_qBq$ Seasonal components:\n$SAR(P): Φ(Bᵐ) = 1 - Φ₁Bᵐ - Φ₂B²ᵐ - \u0026hellip; - Φ_PBᴾᵐ$ $SMA(Q): Θ(Bᵐ) = 1 + Θ₁Bᵐ + Θ₂B²ᵐ + \u0026hellip; + Θ_QBQᵐ$ Differencing:\nNon-seasonal: $(1-B)ᵈ$ Seasonal: $(1-Bᵐ)D$ The complete SARIMA model can be written as:\n$$φ(B)Φ(Bᵐ)(1-B)ᵈ(1-Bᵐ)D Yₜ = θ(B)Θ(Bᵐ)εₜ$$\nWhere:\nB is the backshift operator ($BYₜ = Yₜ₋₁$) Yₜ is the time series εₜ is white noise 4. SARIMA Model Implementation Now that we understand the components and mathematical formulation, let\u0026rsquo;s implement the SARIMA model: From the previous article we found that the best (p,d,q)\nfrom pmdarima.arima.utils import nsdiffs D = nsdiffs(ts, m=12, # commonly requires knowledge of dataset max_D=12, ) # -\u0026gt; 0 D 0\n# Create Training and Test train = ts[:int(len(df)*0.8)] test = ts[int(len(df)*0.8):] def grid_search_sarima(train, test, p_range, d_range, q_range, P_range, D_range, Q_range, m_range): best_aic = float(\u0026#39;inf\u0026#39;) best_mape = float(\u0026#39;inf\u0026#39;) best_order = None for p in p_range: for d in d_range: for q in q_range: for P in P_range: for D in D_range: for Q in Q_range: for m in m_range: try: model = SARIMAX(train.values, order=(p,d,q), seasonal_order=(P,D,Q,m)) results = model.fit() fc_series = pd.Series(results.forecast(steps=len(test)), index=test.index) # 95% conf test_metrics = forecast_accuracy(fc_series.values, test.values) # if results.aic \u0026lt; best_aic: # best_aic = results.aic # best_order = (p,d,q) # print(f\u0026#34;(p,d,q)=({p},{d},{q}) | (P,D,Q,m)=({P},{D},{Q},{m})\u0026#34;, test_metrics[\u0026#39;mape\u0026#39;]) if test_metrics[\u0026#39;mape\u0026#39;] \u0026lt; best_mape: best_mape = test_metrics[\u0026#39;mape\u0026#39;] best_order = (p,d,q) best_sorder = (P,D,Q,m) print(\u0026#34;temp best:\u0026#34; + f\u0026#34;(p,d,q)=({p},{d},{q}) | (P,D,Q,m)=({P},{D},{Q},{m})\u0026#34;, round(test_metrics[\u0026#39;mape\u0026#39;], 4)) except Exception as e: print(e) continue return best_order, best_sorder # Accuracy metrics def forecast_accuracy(forecast, actual): mape = np.mean(np.abs(forecast - actual)/np.abs(actual)) # MAPE me = np.mean(forecast - actual) # ME mae = np.mean(np.abs(forecast - actual)) # MAE mpe = np.mean((forecast - actual)/actual) # MPE rmse = np.mean((forecast - actual)**2)**.5 # RMSE corr = np.corrcoef(forecast, actual)[0,1] # corr mins = np.amin(np.hstack([forecast[:,None], actual[:,None]]), axis=1) maxs = np.amax(np.hstack([forecast[:,None], actual[:,None]]), axis=1) minmax = 1 - np.mean(mins/maxs) # minmax acf1 = acf(forecast-test)[1] # ACF1 return({\u0026#39;mape\u0026#39;:mape, \u0026#39;me\u0026#39;:me, \u0026#39;mae\u0026#39;: mae, \u0026#39;mpe\u0026#39;: mpe, \u0026#39;rmse\u0026#39;:rmse, \u0026#39;acf1\u0026#39;:acf1, \u0026#39;corr\u0026#39;:corr, \u0026#39;minmax\u0026#39;:minmax}) # Determine optimal SARIMA parameters # model = auto_arima(ts, seasonal=True, m=12, # start_p=0, start_q=0, start_P=1, start_Q=1, # max_p=4, max_q=4, max_P=2, max_Q=2, d=1, D=1, # trace=True, error_action=\u0026#39;ignore\u0026#39;, suppress_warnings=True, stepwise=True, out_of_sample=len(test)) # print(model.summary()) # Custom Grid Search # d = 1 # from # best_order, best_sorder = grid_search_sarima(train, test, range(0,3), [d], range(0,3), range(3), [D], range(3), [4, 8, 12]) # print(f\u0026#34;Best ARIMA order based on grid search: {best_order}\u0026#34;) # Fit the SARIMA model # sarima_model = SARIMAX(ts, order=model.order, seasonal_order=model.seasonal_order) (2,1,2) | (P,D,Q,m)=(2,1,1, # sarima_model = SARIMAX(train, order=best_order, seasonal_order=best_sorder) #(2,1,2) | (P,D,Q,m)=(2,1,1,8) # print(best_order, best_sorder) # sarima_model = SARIMAX(train, order=(2,1,2), seasonal_order=(6,0,6,8)) sarima_model = SARIMAX(train, order=(2,1,2), seasonal_order=(2,2,1,8)) results = sarima_model.fit() print(results.summary()) SARIMAX Results ===========================================================================================\rDep. Variable: Close No. Observations: 412\rModel: SARIMAX(2, 1, 2)x(2, 2, [1], 8) Log Likelihood -1073.473\rDate: Fri, 05 Jul 2024 AIC 2162.947\rTime: 20:16:14 BIC 2194.778\rSample: 0 HQIC 2175.558\r- 412 Covariance Type: opg ==============================================================================\rcoef std err z P\u0026gt;|z| [0.025 0.975]\r------------------------------------------------------------------------------\rar.L1 0.0464 0.051 0.902 0.367 -0.054 0.147\rar.L2 0.8356 0.048 17.504 0.000 0.742 0.929\rma.L1 -0.0003 290.396 -1.14e-06 1.000 -569.165 569.164\rma.L2 -0.9997 290.178 -0.003 0.997 -569.739 567.739\rar.S.L8 -0.6368 0.047 -13.574 0.000 -0.729 -0.545\rar.S.L16 -0.2957 0.042 -7.043 0.000 -0.378 -0.213\rma.S.L8 -1.0000 3741.746 -0.000 1.000 -7334.687 7332.687\rsigma2 11.5312 4.29e+04 0.000 1.000 -8.4e+04 8.4e+04\r===================================================================================\rLjung-Box (L1) (Q): 0.14 Jarque-Bera (JB): 23.66\rProb(Q): 0.71 Prob(JB): 0.00\rHeteroskedasticity (H): 0.40 Skew: -0.04\rProb(H) (two-sided): 0.00 Kurtosis: 4.20\r===================================================================================\rWarnings:\r[1] Covariance matrix calculated using the outer product of gradients (complex-step).\r# sarima_model = SARIMAX(train, order=(2,1,2), seasonal_order=(2,2,1,8)) # results = sarima_model.fit() # Forecast forecast_steps = len(test) # Forecast for one year forecast = results.get_forecast(steps=forecast_steps) forecast_ci = forecast.conf_int(alpha=0.25) # forecast_index = pd.date_range(start=test.index[0]+pd.Timedelta(24, \u0026#39;hours\u0026#39;), periods=len(forecast_ci)) forecast_index = test.index # Plot the forecast plt.figure(figsize=(12, 6)) plt.plot(train[-200:], label=\u0026#39;training\u0026#39;) # plt.plot(ts.index, ts, label=\u0026#39;Observed\u0026#39;) plt.plot(test, label=\u0026#39;actual\u0026#39;) plt.plot(forecast_index, forecast.predicted_mean + decomposition.seasonal[test.index].values*10, color=\u0026#39;r\u0026#39;, label=\u0026#39;forecast\u0026#39;) plt.fill_between(forecast_index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], color=\u0026#39;pink\u0026#39;, alpha=0.4) plt.title(f\u0026#39;SARIMA Forecast of {ticker} Stock Prices\u0026#39;) plt.xlabel(\u0026#39;Date\u0026#39;) plt.ylabel(\u0026#39;Price\u0026#39;) plt.legend() plt.show() As the previous plot shows, SARIMA model takes into account stagionality with respect to the ARIMA model and provides better accuracy and mape score (5% vs. 7%). Moreover, the stagionality can deal with temporary detachment from the mean value. Nonetheless, SARIMA model is slower to train and to interfer (respectively $\\approx 100\\% $ and $50\\%$ more). It still does not involve exogenous variables (possible with SARIMAX) and potential outbreaks (like Covid-19 or holidays).\n5. Model Diagnostics After fitting the model, it\u0026rsquo;s important to check its adequacy:\n# Model diagnostics results.plot_diagnostics(figsize=(12, 8)) plt.show() These diagnostic plots help us check for:\nStandardized residual: Should resemble white noise Histogram plus KDE: Should be normally distributed Q-Q plot: Should follow the straight line Correlogram: Should show no significant autocorrelation Conclusion The SARIMA model provides a powerful tool for analyzing and forecasting time series data with both trend and seasonal components. By decomposing the Apple stock price series and applying a SARIMA model, we\u0026rsquo;ve gained insights into the underlying patterns and potential future movements of the stock.\nKey takeaways:\nSignal decomposition revealed clear trend and seasonal components in Apple\u0026rsquo;s stock price. The SARIMA model captures both non-seasonal and seasonal patterns in the data. Model diagnostics are crucial for ensuring the validity of our forecasts. Remember that while these models can provide valuable insights, stock prices are influenced by many external factors not captured in historical data alone. Always combine statistical analysis with fundamental research and an understanding of market conditions when making investment decisions.\n","date":"July 4, 2024","hero":"/posts/finance/stock_prediction/sarima/images/sarima_example_9_1.png","permalink":"http://localhost:1313/posts/finance/stock_prediction/sarima/","summary":"Introduction The Seasonal Autoregressive Integrated Moving Average (SARIMA) model is an extension of the ARIMA model (discussed in the previous article) that incorporates seasonality. This makes it particularly useful for analyzing financial time series data, which often exhibits both trend and seasonal patterns. In this article, we\u0026rsquo;ll apply the SARIMA model to Apple (AAPL) stock data, perform signal decomposition, and provide a detailed mathematical explanation of the model.\n1. Data Preparation and Exploration First, let\u0026rsquo;s obtain the Apple stock data and prepare it for analysis:","tags":["Finance","Statistics","Forecasting"],"title":"Time Series Analysis and SARIMA Model for Stock Price Prediction"},{"categories":["Physics"],"contents":"\rIntroduction Quantum teleportation is a fundamental protocol in quantum information science that enables the transfer of quantum information from one location to another. Despite its name, it doesn\u0026rsquo;t involve the transportation of matter, but rather the transmission of the quantum state of a particle.\nThe Concept In quantum teleportation, we have three main parties:\nAlice: The sender who wants to transmit a quantum state. Bob: The receiver who will receive the quantum state. A quantum channel: Usually an entangled pair of qubits shared between Alice and Bob. The goal is for Alice to transmit the state of her qubit to Bob using only classical communication and their shared entanglement.\nMathematical Formulation Let\u0026rsquo;s walk through the mathematical formulation of quantum teleportation:\nInitial state: Alice has a qubit in an unknown state $|ψ⟩ = α|0⟩ + β|1⟩$, where $|α|² + |β|² = 1$. Alice and Bob share an entangled pair in the Bell state $|Φ⁺⟩ = (1/√2)(|00⟩ + |11⟩)$.\nThe initial state of the entire system: $$|Ψ₀⟩ = |ψ⟩ ⊗ |Φ⁺⟩ = (1/√2)(α|0⟩ + β|1⟩) ⊗ (|00⟩ + |11⟩)$$\nExpanding the state: $$|Ψ₀⟩ = (1/√2)[α|000⟩ + α|011⟩ + β|100⟩ + β|111⟩]$$\nAlice applies a CNOT gate to her qubits: $$|Ψ₁⟩ = (1/√2)[α|000⟩ + α|011⟩ + β|110⟩ + β|101⟩]$$\nAlice applies a Hadamard gate to her first qubit: $$|Ψ₂⟩ = (1/2)[α(|000⟩ + |100⟩) + α(|011⟩ + |111⟩) + β(|010⟩ - |110⟩) + β(|001⟩ - |101⟩)]$$\nRearranging terms: $$|Ψ₂⟩ = (1/2)[|00⟩(α|0⟩ + β|1⟩) + |01⟩(α|1⟩ + β|0⟩) + |10⟩(α|0⟩ - β|1⟩) + |11⟩(α|1⟩ - β|0⟩)]$$\nAlice measures her qubits, collapsing the state. There are four possible outcomes:\n00: Bob\u0026rsquo;s qubit is in state $α|0⟩ + β|1⟩$ 01: Bob\u0026rsquo;s qubit is in state $α|1⟩ + β|0⟩$ 10: Bob\u0026rsquo;s qubit is in state $α|0⟩ - β|1⟩$ 11: Bob\u0026rsquo;s qubit is in state $α|1⟩ - β|0⟩$ Based on Alice\u0026rsquo;s measurement, Bob applies the appropriate correction:\n00: I (identity, do nothing) 01: X (bit flip) 10: Z (phase flip) 11: ZX (bit and phase flip) After Bob\u0026rsquo;s correction, his qubit is in the state $α|0⟩ + β|1⟩$, which is the original state of Alice\u0026rsquo;s qubit.\nThe Protocol: Step-by-Step Preparation:\nAlice has a qubit in state $|ψ⟩ = α|0⟩ + β|1⟩$. Alice and Bob share an entangled pair in the Bell state $|Φ⁺⟩ = (1/√2)(|00⟩ + |11⟩)$. Alice\u0026rsquo;s operations:\nAlice applies a CNOT gate with her qubit as control and her half of the entangled pair as target. Alice applies a Hadamard gate to her qubit. Measurement:\nAlice measures both of her qubits in the computational basis. Classical communication:\nAlice sends the two classical bits resulting from her measurement to Bob. Bob\u0026rsquo;s correction:\nBased on the classical bits received, Bob applies the appropriate quantum gate(s) to his qubit. Result:\nBob\u0026rsquo;s qubit is now in the state $α|0⟩ + β|1⟩$, the original state of Alice\u0026rsquo;s qubit. Implementation in Qiskit Let\u0026rsquo;s implement the quantum teleportation protocol using Qiskit:\nfrom qiskit import QuantumCircuit, QuantumRegister, ClassicalRegister from qiskit_aer import AerSimulator from qiskit.visualization import plot_histogram from qiskit.result import marginal_distribution from qiskit.circuit.library import UGate from numpy import pi, random import matplotlib.pyplot as plt import seaborn as sns # Create quantum and classical registers qubit = QuantumRegister(1, \u0026#34;Q\u0026#34;) ebit0 = QuantumRegister(1, \u0026#34;A\u0026#34;) ebit1 = QuantumRegister(1, \u0026#34;B\u0026#34;) a = ClassicalRegister(1, \u0026#34;a\u0026#34;) b = ClassicalRegister(1, \u0026#34;b\u0026#34;) # Create quantum circuit protocol = QuantumCircuit(qubit, ebit0, ebit1, a, b) # Initialize Q to |1⟩ state # protocol.x(qubit) # Prepare entangled pair (Bell state) for teleportation protocol.h(ebit0) protocol.cx(ebit0, ebit1) protocol.barrier() # Alice\u0026#39;s operations protocol.cx(qubit, ebit0) protocol.h(qubit) protocol.barrier() # Alice measures and sends classical bits to Bob protocol.measure(ebit0, a) protocol.measure(qubit, b) protocol.barrier() # Bob uses the classical bits to conditionally apply gates with protocol.if_test((a, 1)): protocol.x(ebit1) with protocol.if_test((b, 1)): protocol.z(ebit1) # Display the circuit display(protocol.draw(\u0026#39;mpl\u0026#39;, scale=2)) This code creates a quantum circuit that implements the quantum teleportation protocol. Let\u0026rsquo;s break down the steps:\nWe create quantum registers for the qubit to be teleported (Q), Alice\u0026rsquo;s entangled qubit (A), and Bob\u0026rsquo;s entangled qubit (B). We also create classical registers to store measurement results.\nWe prepare the entangled pair shared by Alice and Bob using a Hadamard gate and a CNOT gate.\nAlice performs her operations: a CNOT gate with Q as control and A as target, followed by a Hadamard gate on Q.\nAlice measures her qubits and stores the results in classical bits.\nBased on Alice\u0026rsquo;s measurement results, Bob applies X and/or Z gates to his qubit.\nVerifying the Teleportation To verify that the teleportation worked, we can prepare the initial qubit in a specific state, run the teleportation protocol, and then measure Bob\u0026rsquo;s qubit to see if it matches the initial state. Here\u0026rsquo;s how we can do that:\nRandom Gate We\u0026rsquo;ll use a randomly generated single-qubit unitary gate (U gate) to create an arbitrary quantum state. Here\u0026rsquo;s how we create this random gate:\nWhy Add a Random U Gate?\nThe addition of a random U gate serves several important purposes in our verification process:\nGenerality: By using a random state, we\u0026rsquo;re testing the teleportation protocol for a general, arbitrary quantum state, not just specific states like |0⟩, |1⟩, or |+⟩. Robustness: If the protocol works for a randomly chosen state, it\u0026rsquo;s likely to work for all states. This provides a more thorough verification than testing with a few predetermined states. Avoiding Bias: Random testing helps avoid any unintentional bias in our verification process that might arise from choosing specific test states. Simulating Real-World Scenarios: In practical applications, the states we need to teleport are often unknown or arbitrary. Testing with a random state better simulates these real-world scenarios. from qiskit.circuit.library import UGate, XGate, HGate from numpy import pi, random random_gate = UGate( theta=random.random() * 2 * pi, phi=random.random() * 2 * pi, lam=random.random() * 2 * pi, ) xgate = XGate() hgate = HGate() display(random_gate.to_matrix(), xgate.to_matrix()) array([[-0.61747588+0.j , 0.28776308+0.7320628j ],\n[-0.78573506-0.03665989j, -0.19886638-0.58457559j]])\rarray([[0.+0.j, 1.+0.j], [1.+0.j, 0.+0.j]])\n# Create a new circuit including the same bits and qubits used in the # teleportation protocol. test = QuantumCircuit(qubit, ebit0, ebit1, a, b) # Start with the randomly selected gate on Q Q_input = [0, 1] if Q_input == [0, 1]: # |1⟩ state, so append X gate before the random gate test.append(xgate, qubit) # test.barrier() test.append(random_gate, qubit) test.barrier() # Append the entire teleportation protocol from above. test = test.compose(protocol) test.barrier() # Finally, apply the inverse of the random unitary to B and measure. test.append(random_gate.inverse(), ebit1) result = ClassicalRegister(1, \u0026#34;Result\u0026#34;) test.add_register(result) test.measure(ebit1, result) display(test.draw(\u0026#34;mpl\u0026#34;, scale=2)) Let\u0026rsquo;s break down this process:\nWe apply the random U gate to qubit Q (by default is set to |0⟩), creating a random quantum state. We then run the teleportation protocol, which should teleport this random state from Q to B. After teleportation, we apply the inverse of our random U gate to B. Finally, we measure B. If the teleportation worked correctly, applying the inverse of the random U gate to B should return it to the |0⟩ state (or |1⟩ if X operator is applied), and our measurement should always yield 0.\nfrom qiskit_aer import AerSimulator from qiskit.visualization import plot_histogram from qiskit.result import marginal_distribution plt.style.use(\u0026#34;ggplot\u0026#34;) plt.rcParams.update({\u0026#39;figure.figsize\u0026#39;:(8,5), \u0026#39;figure.dpi\u0026#39;:200}) result = AerSimulator().run(test, shots=2048).result() statistics = result.get_counts() display(plot_histogram(statistics)) # Filter statistics to focus on the test result qubit filtered_statistics = marginal_distribution(statistics, [2]) #2 means the leftmost/ bottom qubit (B in this case) display(plot_histogram(filtered_statistics)) Mathematical Analysis of the Verification Let\u0026rsquo;s analyze what happens in each case:\n|0⟩ state:\nInitial state: $|ψ⟩ = \\ket{0} = 1|0⟩ + 0|1⟩$ Expected final state: |0⟩ Expected measurement: 100% |0⟩ |1⟩ state:\nInitial state: $|ψ⟩ = |1⟩ = 0|0⟩ + 1|1⟩$ Expected final state: |1⟩ Expected measurement: 100% |1⟩ |+⟩ state:\nInitial state: $|ψ⟩ = |+⟩ = (1/√2)(|0⟩ + |1⟩)$ Expected final state: |+⟩ Expected measurement: 50% |0⟩, 50% |1⟩ |−⟩ state:\nInitial state: $|ψ⟩ = |−⟩ = (1/√2)(|0⟩ - |1⟩)$ Expected final state: |−⟩ Expected measurement: 50% |0⟩, 50% |1⟩ Note that for the |+⟩ and |−⟩ states, the measurement results will be the same (50-50 split between |0⟩ and |1⟩). To distinguish between these states, we would need to measure in a different basis (e.g., the X basis).\nConclusion Quantum teleportation is a cornerstone protocol in quantum information science. It demonstrates the power of quantum entanglement and how it can be used in conjunction with classical communication to transmit quantum information. While it doesn\u0026rsquo;t allow for faster-than-light communication or the transportation of matter, it\u0026rsquo;s a crucial building block for many quantum communication protocols and quantum computing algorithms.\nThe mathematical formulation reveals the intricate quantum mechanics at play, showing how entanglement and quantum measurement work together to achieve the seemingly impossible task of transmitting a quantum state using only classical communication.\nThe implementation in Qiskit allows us to simulate this protocol and verify its correctness. As quantum hardware continues to improve, we may see practical applications of quantum teleportation in secure communication systems and distributed quantum computing networks.\nFor further details, I suggest this amazing course from IBM: https://learning.quantum.ibm.com/course/basics-of-quantum-information/\n","date":"July 3, 2024","hero":"/posts/physics/quantum_computing/teleportation/images/blog_teleportation.png","permalink":"http://localhost:1313/posts/physics/quantum_computing/teleportation/","summary":"Introduction Quantum teleportation is a fundamental protocol in quantum information science that enables the transfer of quantum information from one location to another. Despite its name, it doesn\u0026rsquo;t involve the transportation of matter, but rather the transmission of the quantum state of a particle.\nThe Concept In quantum teleportation, we have three main parties:\nAlice: The sender who wants to transmit a quantum state. Bob: The receiver who will receive the quantum state.","tags":["Science","Statistics","Quantum","Physics"],"title":"Quantum Computing - Fundementals - Teleportation"},{"categories":["Physics"],"contents":"Introduction to Quantum Computing Quantum computing represents a transformative leap in computational technology. Unlike classical computers, which use bits as the smallest unit of data, quantum computers employ quantum bits, or qubits. These qubits take advantage of the principles of quantum mechanics, allowing for exponentially greater processing power in certain types of computations.\nCore Concepts:\nSuperposition: Unlike classical bits that can be either 0 or 1, qubits can exist in a state that is a superposition of both. This property allows quantum computers to process a massive amount of data simultaneously. Entanglement: Qubits can become entangled, meaning the state of one qubit can depend on the state of another, no matter the distance between them. This interdependence enables complex correlations and faster computation. Quantum Interference: Quantum systems can interfere with themselves, meaning probabilities can add or subtract based on the wave-like nature of qubits. This phenomenon can be harnessed to reduce errors and optimize outcomes in computations. Quantum computers hold the promise of solving problems that are intractable for classical computers, such as factoring large numbers, simulating quantum physics, and optimizing complex systems.\nMathematical Basis of Quantum Computing Quantum computing\u0026rsquo;s power stems from its unique use of quantum mechanics to represent and manipulate data. This section provides a detailed look into the mathematical principles that underpin quantum computing.\nQuantum States and Qubits In quantum computing, information is encoded in quantum states of qubits. A qubit is the quantum analog of a classical bit, but it can exist in a superposition of the two basis states $|0⟩$ and $|1⟩$.\nState Representation: A qubit is represented as a vector in a two-dimensional complex vector space, commonly denoted as $|ψ⟩$. The general state of a qubit is given by: $$ |ψ⟩ = α|0⟩ + β|1⟩ $$ Here, $α$ and $β$ are complex numbers called probability amplitudes, satisfying the normalization condition: $$ |α|^2 + |β|^2 = 1 $$ This condition ensures that the total probability of the qubit being in state $|0⟩$ or $|1⟩$ is 1.\nDirac Notation: The vector form $|ψ⟩$ is often written in Dirac or bra-ket notation, which is a convenient way to describe quantum states. The basis states $|0⟩$ and $|1⟩$ are the eigenstates of the Pauli Z operator (more on this below).\nQuantum Gates and Operations Quantum gates are the building blocks of quantum circuits, analogous to classical logic gates. They are represented by unitary matrices that act on the state vectors of qubits. Unitarity ensures that the operation preserves the total probability (i.e., it remains 1).\n$$ \\begin{equation} \\begin{pmatrix} 0 \u0026amp; 1 \\\\ 1 \u0026amp; 0 \\end{pmatrix} \\end{equation} $$\nPauli Matrices: The Pauli matrices are fundamental in quantum mechanics and quantum computing. They describe basic quantum operations and are used to represent qubit rotations.\nPauli-X (NOT Gate): $ X = \\begin{bmatrix} 0 \u0026amp; 1 \\\\ 1 \u0026amp; 0 \\end{bmatrix} $ This gate flips the state of a qubit: $X|0⟩ = |1⟩$ and $X|1⟩ = |0⟩$.\nPauli-Y: $$ Y = \\begin{pmatrix} 0 \u0026amp; -i \\\\ i \u0026amp; 0 \\end{pmatrix} $$ The Pauli-Y gate applies a phase shift and flips the state with a complex phase factor $i$.\nPauli-Z (Phase Flip): $$ Z = \\begin{pmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \\end{pmatrix} $$ This gate flips the phase of the qubit’s state: $Z|0⟩ = |0⟩$ and $Z|1⟩ = -|1⟩$.\nHadamard Gate: The Hadamard gate creates a superposition of states. It transforms the basis states as follows: $$ H = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix} $$\n$H|0⟩ = \\frac{1}{\\sqrt{2}} (|0⟩ + |1⟩)$ $H|1⟩ = \\frac{1}{\\sqrt{2}} (|0⟩ - |1⟩)$ Controlled NOT (CNOT) Gate: The CNOT gate is a two-qubit operation that flips the state of the target qubit if the control qubit is in state $|1⟩$. $$ CNOT = \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{pmatrix} $$ The matrix operates on a 4-dimensional space corresponding to the possible states of two qubits.\nPhase Gates:\nS Gate: $$ S = \\begin{pmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; i \\end{pmatrix} $$ The S gate introduces a phase shift of $π/2$.\nT Gate: $$ T = \\begin{pmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; e^{iπ/4} \\end{pmatrix} $$ The T gate introduces a phase shift of $π/4$.\nQuantum Measurement Measurement in quantum computing collapses a quantum state into one of the basis states, which can be observed as classical information. The probabilities of the outcomes are determined by the amplitudes of the state vector components.\nMeasurement in the Computational Basis: The outcome of measuring a qubit in the computational basis ${|0⟩, |1⟩}$ is probabilistically determined by: $$ P(0) = |α|^2 \\quad \\text{and} \\quad P(1) = |β|^2 $$ where $ |α|^2 $ and $ |β|^2 $ are the squared magnitudes of the respective probability amplitudes.\nProjective Measurement: Measurement can be described using projection operators. For a qubit state $|ψ⟩ = α|0⟩ + β|1⟩$, the measurement operator for state $|0⟩$ is: $$ P_0 = |0⟩⟨0| $$ After the measurement, the state collapses to $|0⟩$ or $|1⟩$ with the respective probabilities, and the post-measurement state reflects this collapse.\nMeasurement in Different Bases: Measurements can also be performed in other bases, such as the ${|+\\rangle, |-\\rangle}$ basis, where $|+\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle + |1\\rangle)$ and $|-\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle - |1\\rangle)$. The probabilities and post-measurement states differ depending on the chosen basis.\nQuantum Circuit Example with Qiskit To solidify the understanding of these concepts, let’s look at a practical example using Qiskit to implement a simple quantum circuit.\nExample 1 1-qbit system: analyze matrix multiplication and operators effects. Each operation will be evaluated step-by-step for learning purposes :\nApply Hadamant (H) Operator to $|0⟩$ Apply Z or ($\\sigma_z$) Apply T Apply H again Step 1 - Apply H The Hadamant Operator should make the ket0 equally probable for on 0 and 1 state.\ncircuit = QuantumCircuit(1) circuit.h(0) plt = circuit.draw(\u0026#39;mpl\u0026#39;, scale=2) display(plt) ket0 = Statevector([1, 0]) v = ket0.evolve(circuit) v.draw(\u0026#34;latex\u0026#34;) $$\\frac{\\sqrt{2}}{2} |0\\rangle+\\frac{\\sqrt{2}}{2} |1\\rangle$$\nStep 2 - Apply Z The Z operator is phase_flip, hence $\\ket{1}$ coefficient will change sign.\ncircuit = QuantumCircuit(1) circuit.h(0) circuit.z(0) plt = circuit.draw(\u0026#39;mpl\u0026#39;, scale=2) display(plt) ket0 = Statevector([1, 0]) v = ket0.evolve(circuit) v.draw(\u0026#34;latex\u0026#34;) $$\\frac{\\sqrt{2}}{2} |0\\rangle- \\frac{\\sqrt{2}}{2} |1\\rangle$$\nStep 3 - Apply T T operator applies a phase shift of $\\pi/4$, so we expect a change in $\\ket{1}$ coefficient. Precisely that coefficient will multiplied by $\\frac{\\sqrt{2}}{2}(1+i)$\ncircuit = QuantumCircuit(1) circuit.h(0) circuit.z(0) circuit.t(0) plt = circuit.draw(\u0026#39;mpl\u0026#39;, scale=2) display(plt) ket0 = Statevector([1, 0]) v = ket0.evolve(circuit) v.draw(\u0026#34;latex\u0026#34;) $$\\frac{\\sqrt{2}}{2} |0\\rangle+(- \\frac{1}{2} - \\frac{i}{2}) |1\\rangle$$\nStep 4 - Apply H again circuit = QuantumCircuit(1) circuit.h(0) circuit.z(0) circuit.t(0) circuit.h(0) plt = circuit.draw(\u0026#39;mpl\u0026#39;, scale=2) display(plt) ket0 = Statevector([1, 0]) v = ket0.evolve(circuit) v.draw(\u0026#34;latex\u0026#34;) $$(0.1464466094 - 0.3535533906 i) |0\\rangle+(0.8535533906 + 0.3535533906 i) |1\\rangle$$\nprint(v.probabilities_dict()) # Aritmetic Probability n_samples = 5000 statistics = v.sample_counts(n_samples) static_prob = {\u0026#34;0\u0026#34;: statistics[\u0026#39;0\u0026#39;]/n_samples, \u0026#34;1\u0026#34;: statistics[\u0026#34;1\u0026#34;]/n_samples} plot_histogram(statistics, figsize=(12, 8)) print(static_prob) {\u0026lsquo;0\u0026rsquo;: 0.1464466094067262, \u0026lsquo;1\u0026rsquo;: 0.8535533905932733} # arithmetic\n{\u0026lsquo;0\u0026rsquo;: 0.1406, \u0026lsquo;1\u0026rsquo;: 0.8594} # statistical sampling\nExample 2 - Bell State # Import the necessary Qiskit modules from qiskit import QuantumCircuit from qiskit_aer import AerSimulator from qiskit.visualization import plot_histogram # Create a quantum circuit with 2 qubits and 2 classical bits qc = QuantumCircuit(2) # Apply a Hadamard gate to the first qubit qc.h(0) # Apply a CNOT gate with control qubit 0 and target qubit 1 qc.cx(0, 1) # Measure both qubits # qc.measure([0, 1], [0, 1]) # Deaw the circuit qc.draw(\u0026#39;mpl\u0026#39;, scale=3) # Set up six different observables. from qiskit.quantum_info import SparsePauliOp observables_labels = [\u0026#34;ZZ\u0026#34;, \u0026#34;ZI\u0026#34;, \u0026#34;IZ\u0026#34;, \u0026#34;XX\u0026#34;, \u0026#34;XI\u0026#34;, \u0026#34;IX\u0026#34;] observables = [SparsePauliOp(label) for label in observables_labels] observables Optimize from qiskit_aer.primitives import Estimator estimator = Estimator() job = estimator.run([qc]*len(observables), observables) job.result() # post-processing import matplotlib.pyplot as plt values = job.result().values plt.plot(observables_labels, values, \u0026#39;-o\u0026#39;) plt.show() As expect from the Bell State, only the correlated two-qubits observables like (XX or ZZ) have a non-zero probablity.\nConclusion Understanding the mathematical basis of quantum computing is essential for grasping how quantum computers operate. From the representation of qubits to the actions of quantum gates and the process of measurement, these principles form the foundation upon which quantum algorithms are built. This deeper understanding helps appreciate the transformative potential of quantum computing in solving complex problems that classical computers struggle with.\nFor a comprehensive learning experience, explore the delightful course IBM Quantum Computing Learning platform.\nNext Steps In the next articles, we are going to play with more complex and useful quantum circuits to explore the real capabilities of this world.\nTeleportation ","date":"June 30, 2024","hero":"/posts/physics/quantum_computing/introduction/images/bell_state_sphere.png","permalink":"http://localhost:1313/posts/physics/quantum_computing/introduction/","summary":"Introduction to Quantum Computing Quantum computing represents a transformative leap in computational technology. Unlike classical computers, which use bits as the smallest unit of data, quantum computers employ quantum bits, or qubits. These qubits take advantage of the principles of quantum mechanics, allowing for exponentially greater processing power in certain types of computations.\nCore Concepts:\nSuperposition: Unlike classical bits that can be either 0 or 1, qubits can exist in a state that is a superposition of both.","tags":["Science","Statistics","Quantum","Physics"],"title":"Quantum Computing - Fundementals (Part 1)"},{"categories":["Finance"],"contents":"1. Introduction Time series analysis is a fundamental technique in quantitative finance, particularly for understanding and predicting stock price movements. Among the various time series models, ARIMA (Autoregressive Integrated Moving Average) models have gained popularity due to their flexibility and effectiveness in capturing complex patterns in financial data.\nThis article will explore the application of time series analysis and ARIMA models to stock price prediction. We\u0026rsquo;ll cover the theoretical foundations, practical implementation in Python, and critical considerations for using these models in real-world financial scenarios.\n2. Fundamentals of Time Series Analysis Components of a Time Series A time series typically consists of four components:\nTrend: The long-term movement in the series Seasonality: Regular, periodic fluctuations Cyclical: Irregular fluctuations, often related to economic cycles Residual: Random, unpredictable variations Understanding these components is crucial for effective time series modeling.\nStationarity A key concept in time series analysis is stationarity. A stationary time series has constant statistical properties over time, including mean and variance. Many time series models, including ARIMA, assume stationarity. We often need to transform non-stationary data (like most stock price series) to achieve stationarity. Augmented Dickey-Fuller test can be used to check for stationarity, as showed in next sections.\n3. ARIMA Models: Theoretical Background ARIMA models combine three components:\nAR (Autoregressive): The model uses the dependent relationship between an observation and some number of lagged observations. I (Integrated): The use of differencing of raw observations to make the time series stationary. MA (Moving Average): The model uses the dependency between an observation and a residual error from a moving average model applied to lagged observations. The ARIMA model is typically denoted as ARIMA(p,d,q), where:\np is the order of the AR term d is the degree of differencing q is the order of the MA term Mathematical Representation The ARIMA model can be written as:\n$$ Y_t = c + \\varphi_1 Y_{t-1} + \\varphi_2 Y_{t-2} + \u0026hellip; + \\varphi_p Y_{t-p} + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \u0026hellip; + \\theta_q \\epsilon_{t-q} + \\epsilon_t $$\nWhere:\n$Y_t$ is the differenced series (it may have been differenced more than once) c is a constant $\\phi_i$ are the parameters of the autoregressive part $\\theta_i$ are the parameters of the moving average part $\\epsilon_t$ is white noise 4. Implementing ARIMA Models in Python Let\u0026rsquo;s implement an ARIMA model for stock price prediction using Python and the statsmodels library:\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt from statsmodels.tsa.arima.model import ARIMA from statsmodels.tsa.stattools import adfuller from sklearn.metrics import mean_squared_error import yfinance as yf import seaborn as sns # Download stock data ticker = \u0026#34;AAPL\u0026#34; start_date = \u0026#34;2018-01-01\u0026#34; end_date = \u0026#34;2024-06-23\u0026#34; data = yf.download(ticker, start=start_date, end=end_date) # Prepare the data ts = data[\u0026#39;Close\u0026#39;] # Check for stationarity def test_stationarity(timeseries): result = adfuller(timeseries, autolag=\u0026#39;AIC\u0026#39;) print(\u0026#39;ADF Statistic:\u0026#39;, result[0]) print(\u0026#39;p-value:\u0026#39;, result[1]) return result[1] # Plot the time-series plt.figure(figsize=(12,6)) plt.plot(ts.index[:], ts.values[:], label=\u0026#39;Observed\u0026#39;) plt.title(f\u0026#39;{ticker} Stock Price \u0026#39;) # plt.legend() plt.tight_layout() plt.show() p_val = test_stationarity(ts) if p_val \u0026gt; 0.05: # If non-stationary, difference the series ts_diff = ts.diff().dropna() p_val = test_stationarity(ts_diff) d = 1 if p_val \u0026gt; 0.05: ts_diff = ts.diff().diff().dropna() p_val = test_stationarity(ts_diff) d = 2 print(f\u0026#34;\\nd = {d}\u0026#34;) Output:\nd = 1\nThis script downloads stock data, checks for stationarity, fits an ARIMA model, makes predictions, and evaluates the model\u0026rsquo;s performance. In this case, as expected from the plot, the time-series is not stationary. Hence, d has to be greater or equal to 1.\n5. Model Selection and Diagnostic Checking Choosing the right ARIMA model involves selecting appropriate values for p, d, and q. This process often involves:\nAnalyzing ACF and PACF plots: These help in identifying potential AR and MA orders. Grid search: Trying different combinations of p, d, and q and selecting the best based on information criteria like AIC or BIC. Diagnostic checking: Analyzing residuals to ensure they resemble white noise. Finding ARIMA Parameters (p, d, q) Determining the optimal ARIMA parameters involves a combination of statistical tests, visual inspection, and iterative processes. Here\u0026rsquo;s a systematic approach to finding p, d, and q:\nDetermine d (Differencing Order): Use the Augmented Dickey-Fuller test to check for stationarity. If the series is not stationary, difference it and test again until stationarity is achieved. Determine p (AR Order) and q (MA Order): After differencing, use ACF (Autocorrelation Function) and PACF (Partial Autocorrelation Function) plots. The lag where the ACF cuts off indicates the q value. The lag where the PACF cuts off indicates the p value. Fine-tune with Information Criteria: Use AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) to compare different models. Finding d parameter from plots Since, the stationary was already checkd in the previous, this paragraph is useful for graphical and comphrension purpose. Moreover, with autocorrelation parameters, it is possible to find better values of d that the ADF test cannot recognize.\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf plt.rcParams.update({\u0026#39;figure.figsize\u0026#39;:(15,10), \u0026#39;figure.dpi\u0026#39;:80}) # Import data df = data.copy() # Original Series fig, axes = plt.subplots(3, 2, sharex=False) axes[0, 0].plot(df.index, df.Close); axes[0, 0].set_title(\u0026#39;Original Series - \u0026#39;+ticker) plot_acf(df.Close, ax=axes[0, 1], lags=len(df)-1, color=\u0026#39;k\u0026#39;, auto_ylims=True) # 1st Differencing axes[1, 0].plot(df.index, df.Close.diff()); axes[1, 0].set_title(\u0026#39;1st Order Differencing\u0026#39;) plot_acf(df.Close.diff().dropna(), ax=axes[1, 1], lags=len(df)/7-2, color=\u0026#39;k\u0026#39;, auto_ylims=True) # 2nd Differencing axes[2, 0].plot(df.index, df.Close.diff().diff()); axes[2, 0].set_title(\u0026#39;2nd Order Differencing\u0026#39;) plot_acf(df.Close.diff().diff().dropna(), ax=axes[2, 1], lags=len(df)/7-3, color=\u0026#39;k\u0026#39;, auto_ylims=True) plt.tight_layout() plt.show() Indeed, from the plot, d=2 is probably a better solution since we have few coefficient that goes above the confidence threshold.\nFinding p parameter from plots As suggest previously, Partical Correlation Plot is adopted to find the p parameter.\nplt.rcParams.update({\u0026#39;figure.figsize\u0026#39;:(15,5), \u0026#39;figure.dpi\u0026#39;:80}) fig, axes = plt.subplots(1, 2, sharex=False) axes[0].plot(df.index, df.Close.diff()); axes[0].set_title(\u0026#39;1st Differencing\u0026#39;) axes[1].set(ylim=(0,5)) plot_pacf(df.Close.diff().dropna(), ax=axes[1], lags=200, color=\u0026#39;k\u0026#39;, auto_ylims=True, zero=False) plt.tight_layout() plt.show() A possible choice of p can 8 or 18, where the coefficient crosses the confidence intervals.\nFinding q parameter from plots plt.rcParams.update({\u0026#39;figure.figsize\u0026#39;:(15,5), \u0026#39;figure.dpi\u0026#39;:80}) fig, axes = plt.subplots(1, 2, sharex=False) axes[0].plot(df.Close.diff()); axes[0].set_title(\u0026#39;1st Differencing\u0026#39;) axes[1].set(ylim=(0,1.2)) plot_acf(df.Close.diff().dropna(), ax=axes[1], lags=200, color=\u0026#39;k\u0026#39;, auto_ylims=True, zero=False) plt.tight_layout() plt.show() ACF looks very similar to PCF for smaller lags. Hence, even in this case a value of 8 can be used as q.\n6. ARIMA model fitting Predict ARIMA model on all data model = ARIMA(df.Close, order=(8,2,8)) # p,d,q results = model.fit() print(results.summary()) # Actual vs Fitted o plt.plot(results.predict()[-100:], \u0026#39;-*\u0026#39;, label=\u0026#39;prediction\u0026#39;) plt.plot(df.Close[-100:], \u0026#39;-*\u0026#39;, label=\u0026#39;actual\u0026#39;) plt.legend() plt.title(\u0026#34;Prediction vs. Actual on All Data \u0026#34;) plt.tight_layout() plt.show() Train/ Test split from statsmodels.tsa.stattools import acf # Create Training and Test train = df.Close[:int(len(df)*0.8)] test = df.Close[int(len(df)*0.8):] # model = ARIMA(train, order=(3,2,1)) model = ARIMA(train, order=(8, 2, 8)) fitted = model.fit() # Forecast fc = fitted.get_forecast(steps=len(test), alpha=0.05) # 95% conf conf = fc.conf_int() # Make as pandas series fc_series = pd.Series(fitted.forecast(steps=len(test)).values, index=test.index) lower_series = pd.Series(conf.iloc[:, 0].values, index=test.index) upper_series = pd.Series(conf.iloc[:, 1].values, index=test.index) # Plot plt.figure(figsize=(12,5), dpi=100) plt.plot(train[-200:], label=\u0026#39;training\u0026#39;) plt.plot(test, label=\u0026#39;actual\u0026#39;) plt.plot(fc_series, label=\u0026#39;forecast\u0026#39;) plt.fill_between(lower_series.index, lower_series, upper_series, color=\u0026#39;k\u0026#39;, alpha=.15) plt.title(\u0026#39;Forecast vs Actuals\u0026#39;) plt.legend(loc=\u0026#39;upper left\u0026#39;, fontsize=10) plt.tight_layout() plt.show() # Accuracy metrics def forecast_accuracy(forecast, actual): mape = np.mean(np.abs(forecast - actual)/np.abs(actual)) # MAPE me = np.mean(forecast - actual) # ME mae = np.mean(np.abs(forecast - actual)) # MAE mpe = np.mean((forecast - actual)/actual) # MPE rmse = np.mean((forecast - actual)**2)**.5 # RMSE corr = np.corrcoef(forecast, actual)[0,1] # corr mins = np.amin(np.hstack([forecast[:,None], actual[:,None]]), axis=1) maxs = np.amax(np.hstack([forecast[:,None], actual[:,None]]), axis=1) minmax = 1 - np.mean(mins/maxs) # minmax acf1 = acf(forecast-test)[1] # ACF1 return({\u0026#39;mape\u0026#39;:mape, \u0026#39;me\u0026#39;:me, \u0026#39;mae\u0026#39;: mae, \u0026#39;mpe\u0026#39;: mpe, \u0026#39;rmse\u0026#39;:rmse, \u0026#39;acf1\u0026#39;:acf1, \u0026#39;corr\u0026#39;:corr, \u0026#39;minmax\u0026#39;:minmax}) forecast_accuracy(fc_series.values, test.values) Output:\n{\u0026lsquo;mape\u0026rsquo;: 0.07829701788549515,\n\u0026lsquo;me\u0026rsquo;: -12.898037657120996,\n\u0026lsquo;mae\u0026rsquo;: 14.483068468837455,\n\u0026lsquo;mpe\u0026rsquo;: -0.068860507560246,\n\u0026lsquo;rmse\u0026rsquo;: 16.906382957008496,\n\u0026lsquo;acf1\u0026rsquo;: 0.9702976318229376,\n\u0026lsquo;corr\u0026rsquo;: 0.4484875181364141,\n\u0026lsquo;minmax\u0026rsquo;: 0.07810488835602647}\nGrid Search def grid_search_arima(train, test, p_range, d_range, q_range): best_aic = float(\u0026#39;inf\u0026#39;) best_mape = float(\u0026#39;inf\u0026#39;) best_order = None for p in p_range: for d in d_range: for q in q_range: try: model = ARIMA(train.values, order=(p,d,q)) results = model.fit() fc_series = pd.Series(results.forecast(steps=len(test)), index=test.index) # 95% conf test_metrics = forecast_accuracy(fc_series.values, test.values) # if results.aic \u0026lt; best_aic: # best_aic = results.aic # best_order = (p,d,q) print(p,d,q, test_metrics[\u0026#39;mape\u0026#39;]) if test_metrics[\u0026#39;mape\u0026#39;] \u0026lt; best_mape: best_mape = test_metrics[\u0026#39;mape\u0026#39;] best_order = (p,d,q) print(\u0026#34;temp best:\u0026#34;, best_order, test_metrics[\u0026#39;mape\u0026#39;]) except Exception as e: print(e) continue return best_order # Grid search for best p and q (assuming d is known) best_order = grid_search_arima(train, test, range(1,9), [d, d+1], range(1,9)) print(f\u0026#34;Best ARIMA order based on grid search: {best_order}\u0026#34;) Suggested d value: 1\ntemp best: (1, 1, 1) 0.14570196898952395\ntemp best: (1, 1, 5) 0.14514639508226412\ntemp best: (1, 1, 6) 0.14499024417142595\ntemp best: (1, 1, 7) 0.1439625731680348\ntemp best: (1, 2, 1) 0.07729490750827837\ntemp best: (1, 2, 2) 0.0764917667521908\ntemp best: (3, 2, 4) 0.07647187068962996\nBest ARIMA order based on grid search: (3, 2, 4)\nAt the end, we found that (3,2,4) offer better testing results (mape of 7.6% vs. 7.8% of manual parameters finding), even if there is only a tiny difference (0.2%). Moreover, since the values of p and q are lower the model will be faster and less prone to overfitting (fewer number of AR or MA coefficients).\n7. Limitations and Considerations While ARIMA models can be powerful for time series prediction, they have limitations:\nAssumption of linearity: ARIMA models assume linear relationships, which may not hold for complex financial data. Limited forecasting horizon: They tend to perform poorly for long-term forecasts. Sensitivity to outliers: Extreme values can significantly impact model performance. Assumption of constant variance: This may not hold for volatile stock prices. No consideration of external factors: ARIMA models only use past values of the time series, ignoring other potentially relevant variables. 8. Conclusion Time series analysis and ARIMA models provide valuable tools for understanding and predicting stock price movements. While they have limitations, particularly in the complex and often non-linear world of financial markets, they serve as a strong foundation for more advanced modeling techniques.\nWhen applying these models to real-world financial data, it\u0026rsquo;s crucial to:\nThoroughly understand the underlying assumptions Carefully preprocess and analyze the data Conduct rigorous model selection and diagnostic checking Interpret results with caution, considering the model\u0026rsquo;s limitations Combine with other analytical techniques and domain expertise for comprehensive analysis As with all financial modeling, remember that past performance does not guarantee future results. Time series models should be one tool in a broader analytical toolkit, complemented by fundamental analysis, market sentiment assessment, and a deep understanding of the specific stock and its market context.\nNext Steps In next articles, we are going to explore about time-series decomposition, seasanality, exogenous variables. Indeed, several extensions to basic ARIMA models address some of these limitations:\nSARIMA: Incorporates seasonality. ARIMAX: Includes exogenous variables. GARCH: Models time-varying volatility. Vector ARIMA: Handles multiple related time series simultaneously. ","date":"June 28, 2024","hero":"/posts/finance/stock_prediction/arima/images/test_forecast.png","permalink":"http://localhost:1313/posts/finance/stock_prediction/arima/","summary":"1. Introduction Time series analysis is a fundamental technique in quantitative finance, particularly for understanding and predicting stock price movements. Among the various time series models, ARIMA (Autoregressive Integrated Moving Average) models have gained popularity due to their flexibility and effectiveness in capturing complex patterns in financial data.\nThis article will explore the application of time series analysis and ARIMA models to stock price prediction. We\u0026rsquo;ll cover the theoretical foundations, practical implementation in Python, and critical considerations for using these models in real-world financial scenarios.","tags":["Finance","Statistics","Forecasting"],"title":"Time Series Analysis and ARIMA Models for Stock Price Prediction"},{"categories":["Computer Vision"],"contents":"Install dependencies Type the following command to install possible needed dependencies (especially if the inference is performed on the CPU)\n%pip install einops flash_attn In Kaggle, transformers and torch are already installed. Otherwise you also need to install them on your local PC.\nImport Libraries from transformers import AutoProcessor, AutoModelForCausalLM from PIL import Image import requests import copy import torch %matplotlib inline Import the model We can choose Florence-2-large or Florence-2-large-ft (fine-tuned).\nmodel_id = \u0026#39;microsoft/Florence-2-large-ft\u0026#39; device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) print(device) model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).eval() model = model.to(device) # put the model on the available GPU processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) Define inference function def run_inference(task_prompt, text_input=None): if text_input is None: prompt = task_prompt else: prompt = task_prompt + text_input inputs = processor(text=prompt, images=image, return_tensors=\u0026#34;pt\u0026#34;).to(device) generated_ids = model.generate( input_ids=inputs[\u0026#34;input_ids\u0026#34;], pixel_values=inputs[\u0026#34;pixel_values\u0026#34;], max_new_tokens=1024, early_stopping=False, do_sample=False, num_beams=3, ) generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0] parsed_answer = processor.post_process_generation( generated_text, task=task_prompt, image_size=(image.width, image.height) ) return parsed_answer Get image link image_url = \u0026#34;http://lerablog.org/wp-content/uploads/2013/06/two-cars.jpg\u0026#34; # an arbitrary image link or filepath can be inserted here image = Image.open(requests.get(image_url, stream=True).raw) image Run pre-defined tasks without additional inputs Caption task_prompt = \u0026#39;\u0026lt;CAPTION\u0026gt;\u0026#39; run_inference(task_prompt) {\u0026rsquo;\u0026rsquo;: \u0026lsquo;Two sports cars parked next to each other on a road.\u0026rsquo;}\ntask_prompt = \u0026#39;\u0026lt;DETAILED_CAPTION\u0026gt;\u0026#39; run_inference(task_prompt) {\u0026rsquo;\u0026lt;DETAILED_CAPTION\u0026gt;\u0026rsquo;: \u0026lsquo;In this image we can see two cars on the road. In the background, we can also see water, hills and the sky.\u0026rsquo;}\ntask_prompt = \u0026#39;\u0026lt;MORE_DETAILED_CAPTION\u0026gt;\u0026#39; run_inference(task_prompt) {\u0026rsquo;\u0026lt;MORE_DETAILED_CAPTION\u0026gt;\u0026rsquo;: \u0026lsquo;There are two cars parked on the street. There is water behind the cars. There are mountains behind the water. The cars are yellow and black. \u0026lsquo;}\nObject Detection task_prompt = \u0026#39;\u0026lt;OD\u0026gt;\u0026#39; results = run_inference(task_prompt) print(results) import matplotlib.pyplot as plt import matplotlib.patches as patches def plot_bbox(image, data): # Create a figure and axes fig, ax = plt.subplots() # Display the image ax.imshow(image) # Plot each bounding box for bbox, label in zip(data[\u0026#39;bboxes\u0026#39;], data[\u0026#39;labels\u0026#39;]): # Unpack the bounding box coordinates x1, y1, x2, y2 = bbox # Create a Rectangle patch rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor=\u0026#39;r\u0026#39;, facecolor=\u0026#39;none\u0026#39;) # Add the rectangle to the Axes ax.add_patch(rect) # Annotate the label plt.text(x1, y1, label, color=\u0026#39;white\u0026#39;, fontsize=8, bbox=dict(facecolor=\u0026#39;red\u0026#39;, alpha=0.5)) # Remove the axis ticks and labels ax.axis(\u0026#39;off\u0026#39;) # Show the plot plt.show() plot_bbox(image, results[\u0026#39;\u0026lt;OD\u0026gt;\u0026#39;]) Dense Region Caption task_prompt = \u0026#39;\u0026lt;DENSE_REGION_CAPTION\u0026gt;\u0026#39; results = run_inference(task_prompt) dense_region_res = results print(results) {\u0026rsquo;\u0026lt;DENSE_REGION_CAPTION\u0026gt;\u0026rsquo;: {\u0026lsquo;bboxes\u0026rsquo;: [[334.8450012207031, 115.95000457763672, 599.4450073242188, 248.5500030517578], [18.584999084472656, 117.45000457763672, 304.6050109863281, 236.\u0026gt; 25001525878906], [113.71499633789062, 177.15000915527344, 172.30499267578125, 235.95001220703125], [404.1449890136719, 187.95001220703125, 453.9150085449219, 248.25001525878906], [26.\u0026gt; 774999618530273, 173.85000610351562, 73.3949966430664, 228.15000915527344], [336.1050109863281, 176.25, 380.2049865722656, 235.95001220703125], [244.125, 216.45001220703125, 290.7449951171875, 230.85000610351562], [546.5250244140625, 236.5500030517578, 588.7349853515625, 245.85000610351562], [481.635009765625, 148.35000610351562, 509.3550109863281, 157.65000915527344]], \u0026rsquo;labels\u0026rsquo;: [\u0026lsquo;yellow sports car\u0026rsquo;, \u0026lsquo;sports car\u0026rsquo;, \u0026lsquo;wheel\u0026rsquo;, \u0026lsquo;wheel\u0026rsquo;, \u0026lsquo;wheel\u0026rsquo;, \u0026lsquo;wheel\u0026rsquo;, \u0026lsquo;wheel\u0026rsquo;, \u0026lsquo;wheel\u0026rsquo;, \u0026lsquo;wheel\u0026rsquo;]}}\nplot_bbox(image, results[\u0026#39;\u0026lt;DENSE_REGION_CAPTION\u0026gt;\u0026#39;]) Phrase Grounding task_prompt = \u0026#39;\u0026lt;CAPTION_TO_PHRASE_GROUNDING\u0026gt;\u0026#39; results = run_inference(task_prompt, text_input=\u0026#34;Yellow car with islands in background\u0026#34;) print(results) plot_bbox(image, results[\u0026#39;\u0026lt;CAPTION_TO_PHRASE_GROUNDING\u0026gt;\u0026#39;]) {\u0026rsquo;\u0026lt;CAPTION_TO_PHRASE_GROUNDING\u0026gt;\u0026rsquo;: {\u0026lsquo;bboxes\u0026rsquo;: [[335.4750061035156, 115.6500015258789, 601.9649658203125, 250.35000610351562], [0.3149999976158142, 12.15000057220459, 629.0549926757812, 103.6500015258789]], \u0026rsquo;labels\u0026rsquo;: [\u0026lsquo;Yellow car\u0026rsquo;, \u0026lsquo;islands\u0026rsquo;]}} Segmentation task_prompt = \u0026#39;\u0026lt;REFERRING_EXPRESSION_SEGMENTATION\u0026gt;\u0026#39; results = run_inference(task_prompt, text_input=\u0026#34;yellow car and island\u0026#34;) print(results) from PIL import Image, ImageDraw, ImageFont import random import numpy as np colormap = [\u0026#39;blue\u0026#39;,\u0026#39;orange\u0026#39;,\u0026#39;green\u0026#39;,\u0026#39;purple\u0026#39;,\u0026#39;brown\u0026#39;,\u0026#39;pink\u0026#39;,\u0026#39;gray\u0026#39;,\u0026#39;olive\u0026#39;,\u0026#39;cyan\u0026#39;,\u0026#39;red\u0026#39;, \u0026#39;lime\u0026#39;,\u0026#39;indigo\u0026#39;,\u0026#39;violet\u0026#39;,\u0026#39;aqua\u0026#39;,\u0026#39;magenta\u0026#39;,\u0026#39;coral\u0026#39;,\u0026#39;gold\u0026#39;,\u0026#39;tan\u0026#39;,\u0026#39;skyblue\u0026#39;] def draw_polygons(image, prediction, fill_mask=False): \u0026#34;\u0026#34;\u0026#34; Draws segmentation masks with polygons on an image. Parameters: - image_path: Path to the image file. - prediction: Dictionary containing \u0026#39;polygons\u0026#39; and \u0026#39;labels\u0026#39; keys. \u0026#39;polygons\u0026#39; is a list of lists, each containing vertices of a polygon. \u0026#39;labels\u0026#39; is a list of labels corresponding to each polygon. - fill_mask: Boolean indicating whether to fill the polygons with color. \u0026#34;\u0026#34;\u0026#34; # Load the image draw = ImageDraw.Draw(image) # Set up scale factor if needed (use 1 if not scaling) scale = 1 # Iterate over polygons and labels for polygons, label in zip(prediction[\u0026#39;polygons\u0026#39;], prediction[\u0026#39;labels\u0026#39;]): color = random.choice(colormap) fill_color = random.choice(colormap) if fill_mask else None for _polygon in polygons: _polygon = np.array(_polygon).reshape(-1, 2) if len(_polygon) \u0026lt; 3: print(\u0026#39;Invalid polygon:\u0026#39;, _polygon) continue _polygon = (_polygon * scale).reshape(-1).tolist() # Draw the polygon if fill_mask: draw.polygon(_polygon, outline=color, fill=fill_color) else: draw.polygon(_polygon, outline=color) # Draw the label text draw.text((_polygon[0] + 8, _polygon[1] + 2), label, fill=color) # Save or display the image #image.show() # Display the image display(image) output_image = copy.deepcopy(image) draw_polygons(output_image, results[\u0026#39;\u0026lt;REFERRING_EXPRESSION_SEGMENTATION\u0026gt;\u0026#39;], fill_mask=True) Regions Segmentation def bbox_to_loc(bbox): # bbox position need to be rescaled from 0 to 999. the coordinates are x1_y1_x2_y2 return f\u0026#34;\u0026lt;loc_{int(bbox[0]*999/width)}\u0026gt;\u0026lt;loc_{int(bbox[1]*999/height)}\u0026gt;\u0026lt;loc_{int(bbox[2]*999/width)}\u0026gt;\u0026lt;loc_{int(bbox[3]*999/height)}\u0026gt;\u0026#34; with torch.no_grad(): torch.cuda.empty_cache() output_image = copy.deepcopy(image) height, width = image.height, image.width task_prompt = \u0026#39;\u0026lt;REGION_TO_SEGMENTATION\u0026gt;\u0026#39; for bbox in dense_region_res[\u0026#39;\u0026lt;DENSE_REGION_CAPTION\u0026gt;\u0026#39;][\u0026#39;bboxes\u0026#39;][:]: print(bbox_to_loc(bbox)) results = run_inference(task_prompt, text_input=bbox_to_loc(bbox)) draw_polygons(output_image, results[task_prompt], fill_mask=True) plot_bbox(output_image, dense_region_res[\u0026#39;\u0026lt;DENSE_REGION_CAPTION\u0026gt;\u0026#39;]) \u0026lt;loc_530\u0026gt;\u0026lt;loc_386\u0026gt;\u0026lt;loc_950\u0026gt;\u0026lt;loc_827\u0026gt; \u0026lt;loc_29\u0026gt;\u0026lt;loc_391\u0026gt;\u0026lt;loc_483\u0026gt;\u0026lt;loc_786\u0026gt; \u0026lt;loc_180\u0026gt;\u0026lt;loc_589\u0026gt;\u0026lt;loc_273\u0026gt;\u0026lt;loc_785\u0026gt; \u0026lt;loc_640\u0026gt;\u0026lt;loc_625\u0026gt;\u0026lt;loc_719\u0026gt;\u0026lt;loc_826\u0026gt; \u0026lt;loc_42\u0026gt;\u0026lt;loc_578\u0026gt;\u0026lt;loc_116\u0026gt;\u0026lt;loc_759\u0026gt; \u0026lt;loc_532\u0026gt;\u0026lt;loc_586\u0026gt;\u0026lt;loc_602\u0026gt;\u0026lt;loc_785\u0026gt; \u0026lt;loc_387\u0026gt;\u0026lt;loc_720\u0026gt;\u0026lt;loc_461\u0026gt;\u0026lt;loc_768\u0026gt; \u0026lt;loc_866\u0026gt;\u0026lt;loc_787\u0026gt;\u0026lt;loc_933\u0026gt;\u0026lt;loc_818\u0026gt; \u0026lt;loc_763\u0026gt;\u0026lt;loc_494\u0026gt;\u0026lt;loc_807\u0026gt;\u0026lt;loc_524\u0026gt; OCR url = \u0026#34;https://m.media-amazon.com/images/I/510sf0pRTlL.jpg\u0026#34; image = Image.open(requests.get(url, stream=True).raw).convert(\u0026#39;RGB\u0026#39;) image task_prompt = \u0026#39;\u0026lt;OCR_WITH_REGION\u0026gt;\u0026#39; results = run_inference(task_prompt) print(results) {\u0026rsquo;\u0026lt;OCR_WITH_REGION\u0026gt;\u0026rsquo;: {\u0026lsquo;quad_boxes\u0026rsquo;: [[143.8125, 146.25, 280.9624938964844, 146.25, 280.9624938964844, 172.25, 143.8125, 172.25], [134.0625, 176.25, 281.9375, 176.25, 281.9375, 202.25, 134.0625, 202.25], [172.73748779296875, 206.25, 284.2124938964844, 206.25, 284.2124938964844, 216.25, 172.73748779296875, 216.25], [150.3125, 238.25, 281.9375, 238.25, 281.9375, 247.25, 150.3125, 247.25], [139.58749389648438, 254.25, 284.2124938964844, 254.25, 284.2124938964844, 277.75, 139.58749389648438, 277.75], [133.08749389648438, 283.75, 285.1875, 283.75, 285.1875, 307.75, 133.08749389648438, 307.75], [140.5625, 312.75, 281.9375, 312.75, 281.9375, 320.75, 140.5625, 320.75]], \u0026rsquo;labels\u0026rsquo;: [\u0026rsquo;QUANTUM\u0026rsquo;, \u0026lsquo;MECHANICS\u0026rsquo;, \u0026lsquo;(Non-relativistic Theory)\u0026rsquo;, \u0026lsquo;Course of Theoretical Phyias Volume 3\u0026rsquo;, \u0026lsquo;L.D. LANDAU\u0026rsquo;, \u0026lsquo;E.M. LIFSHITZ\u0026rsquo;, \u0026lsquo;Initiute of Physical Problems, USSR Academy of\u0026rsquo;]}}\nThe overall extracted text from the image is very close to the original one. However, since the image resolution is low, the accuracy on the extracted text is quite low.\ndef draw_ocr_bboxes(image, prediction): scale = 1 draw = ImageDraw.Draw(image) bboxes, labels = prediction[\u0026#39;quad_boxes\u0026#39;], prediction[\u0026#39;labels\u0026#39;] for box, label in zip(bboxes, labels): color = random.choice(colormap) new_box = (np.array(box) * scale).tolist() draw.polygon(new_box, width=3, outline=color) draw.text((new_box[0]-8, new_box[1]-10), \u0026#34;{}\u0026#34;.format(label), align=\u0026#34;right\u0026#34;, fill=color) display(image) output_image = copy.deepcopy(image) draw_ocr_bboxes(output_image, results[\u0026#39;\u0026lt;OCR_WITH_REGION\u0026gt;\u0026#39;]) ","date":"June 25, 2024","hero":"/posts/machine-learning/deep-learning/computer-vision/florence/images/florence-2-lvm-computer-vision-exploration_28_3.png","permalink":"http://localhost:1313/posts/machine-learning/deep-learning/computer-vision/florence/","summary":"Install dependencies Type the following command to install possible needed dependencies (especially if the inference is performed on the CPU)\n%pip install einops flash_attn In Kaggle, transformers and torch are already installed. Otherwise you also need to install them on your local PC.\nImport Libraries from transformers import AutoProcessor, AutoModelForCausalLM from PIL import Image import requests import copy import torch %matplotlib inline Import the model We can choose Florence-2-large or Florence-2-large-ft (fine-tuned).","tags":["Deep Learning","Computer Vision","Machine Learning"],"title":"Florence-2 - Vision Foundation Model - Examples"},{"categories":["Finance"],"contents":"1. Introduction In the dynamic world of finance, options play a crucial role in risk management, speculation, and portfolio optimization. An option is a contract that gives the holder the right, but not the obligation, to buy (call option) or sell (put option) an underlying asset at a predetermined price (strike price) within a specific time frame. The challenge lies in accurately pricing these financial instruments, given the uncertainties in market movements.\nTraditional analytical methods, while powerful, often struggle with complex option structures or realistic market conditions. This is where Monte Carlo simulation steps in, offering a flexible and robust approach to option pricing. By leveraging the power of computational methods, Monte Carlo simulations can handle a wide array of option types and market scenarios, making it an indispensable tool in a quantitative analyst\u0026rsquo;s toolkit.\nFor further explanation about options pricing, check Investopedia.\n2. The Black-Scholes Model Before diving into Monte Carlo methods, it\u0026rsquo;s crucial to understand the Black-Scholes model, a cornerstone in option pricing theory. Developed by Fischer Black, Myron Scholes, and Robert Merton in the early 1970s, this model revolutionized the field of quantitative finance.\nThe Black-Scholes Formula For a European call option, the Black-Scholes formula is:\n$$ C = S₀N(d_1) - Ke^{-rT}N(d_2) $$\nWhere:\n$$ d_1 = \\frac{(ln(S_0/K) + (r + σ²/2)T)}{(σ\\sqrt{T})}, \\quad d_2 = d_1 - \\sigma \\sqrt{T} $$\nC: Call option price S₀: Current stock price K: Strike price r: Risk-free interest rate T: Time to maturity σ: Volatility of the underlying asset N(x): Cumulative standard normal distribution function Assumptions of the Black-Scholes Model The Black-Scholes model rests on several key assumptions:\nThe stock price follows a geometric Brownian motion with constant drift and volatility. No arbitrage opportunities exist in the market. It\u0026rsquo;s possible to buy and sell any amount of stock or options (including fractional amounts). There are no transaction costs or taxes. All securities are perfectly divisible. The risk-free interest rate is constant and known. The underlying stock does not pay dividends. Limitations of the Black-Scholes Model While groundbreaking, the Black-Scholes model has several limitations:\nConstant Volatility: The model assumes volatility is constant, which doesn\u0026rsquo;t hold in real markets where volatility can change dramatically. Log-normal Distribution: It assumes stock returns are normally distributed, which doesn\u0026rsquo;t account for the fat-tailed distributions observed in reality. Continuous Trading: The model assumes continuous trading is possible, which isn\u0026rsquo;t realistic in practice. No Dividends: It doesn\u0026rsquo;t account for dividends, which can significantly affect option prices. European Options Only: The original model only prices European-style options, not American or exotic options. Risk-free Rate: It assumes a constant, known risk-free rate, which can vary in reality. These limitations highlight why more flexible approaches like Monte Carlo simulation are valuable in option pricing.\n3. Monte Carlo Simulation: Theoretical Background Monte Carlo simulation addresses many of the Black-Scholes model\u0026rsquo;s limitations by using computational power to model a wide range of possible future scenarios.\nBasic Principles Monte Carlo methods use repeated random sampling to obtain numerical results. In the context of option pricing, we simulate many possible price paths for the underlying asset and then calculate the option\u0026rsquo;s payoff for each path.\nApplication to Option Pricing For option pricing, we model the stock price movement using a stochastic differential equation:\n$$ dS = \\mu Sdt + \\sigma SdW $$\nWhere:\nS: Stock price μ: Expected return σ: Volatility dW: Wiener process (random walk) This equation is then discretized for simulation purposes.\n4. Implementing Monte Carlo Simulation in Python Let\u0026rsquo;s implement a basic Monte Carlo simulation for pricing a European call option:\nimport numpy as np import matplotlib.pyplot as plt def monte_carlo_option_pricing(S0, K, T, r, sigma, num_simulations, num_steps): dt = T / num_steps paths = np.zeros((num_simulations, num_steps + 1)) paths[:, 0] = S0 for t in range(1, num_steps + 1): z = np.random.standard_normal(num_simulations) paths[:, t] = paths[:, t-1] * np.exp((r - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * z) option_payoffs = np.maximum(paths[:, -1] - K, 0) option_price = np.exp(-r * T) * np.mean(option_payoffs) return option_price, paths # Example usage S0 = 100 # Initial stock price K = 98.5 # Strike price T = 1 # Time to maturity (in years) r = 0.05 # Risk-free rate sigma = 0.2 # Volatility num_simulations = 10000 num_steps = 252 # Number of trading days in a year price, paths = monte_carlo_option_pricing(S0, K, T, r, sigma, num_simulations, num_steps) print(f\u0026#34;Estimated option price: {price:.2f}\u0026#34;) This code simulates multiple stock price paths, calculates the option payoff for each path, and then averages these payoffs to estimate the option price.\n5. Visualization and Analysis Visualizing the results helps in understanding the distribution of possible outcomes:\nplt.figure(figsize=(10, 6)) plt.plot(paths[:100, :].T) plt.title(\u0026#34;Sample Stock Price Paths\u0026#34;) plt.xlabel(\u0026#34;Time Steps\u0026#34;) plt.ylabel(\u0026#34;Stock Price\u0026#34;) plt.show() plt.figure(figsize=(10, 6)) plt.hist(paths[:, -1], bins=50) plt.title(\u0026#34;Distribution of Final Stock Prices\u0026#34;) plt.xlabel(\u0026#34;Stock Price\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.show() These visualizations show the range of possible stock price paths and the distribution of final stock prices, providing insight into the option\u0026rsquo;s potential outcomes.\n6. Comparison with Analytical Solutions To validate our Monte Carlo results, we can compare them with the Black-Scholes analytical solution:\nfrom scipy.stats import norm def black_scholes_call(S0, K, T, r, sigma): d1 = (np.log(S0 / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T)) d2 = d1 - sigma * np.sqrt(T) return S0 * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2) bs_price = black_scholes_call(S0, K, T, r, sigma) print(f\u0026#34;Black-Scholes price: {bs_price:.3f}\u0026#34;) print(f\u0026#34;Monte Carlo price: {price:.3f}\u0026#34;) print(f\u0026#34;Difference: {abs(bs_price - price):.4f}\u0026#34;) The difference between the two methods gives us an idea of the Monte Carlo simulation\u0026rsquo;s accuracy.\nBlack-Scholes price: 11.270\nMonte Carlo price: 11.445\nDifference: 0.1744\n7. Advanced Topics and Extensions Monte Carlo simulation\u0026rsquo;s flexibility allows for various extensions:\nVariance Reduction Techniques: Methods like antithetic variates can improve accuracy without increasing computational cost. Exotic Options: Monte Carlo can price complex options like Asian or barrier options, which are challenging for analytical methods. Incorporating Dividends: We can easily modify the simulation to account for dividend payments. Stochastic Volatility: Models like Heston can be implemented to account for changing volatility. 8. Conclusion Monte Carlo simulation offers a powerful and flexible approach to option pricing, addressing many limitations of analytical methods like the Black-Scholes model. While it can be computationally intensive, it handles complex scenarios and option structures with relative ease.\nThe method\u0026rsquo;s ability to incorporate various market dynamics, such as changing volatility or dividend payments, makes it invaluable in real-world financial modeling. As computational power continues to increase, Monte Carlo methods are likely to play an even more significant role in quantitative finance.\nHowever, it\u0026rsquo;s important to remember that any model, including Monte Carlo simulation, is only as good as its underlying assumptions. Careful consideration of these assumptions and regular validation against market data remain crucial in applying these techniques effectively in practice.\n","date":"June 23, 2024","hero":"/posts/finance/monte_carlo/black-scholes/Option-Pricing-Models-1.jpg","permalink":"http://localhost:1313/posts/finance/monte_carlo/black-scholes/","summary":"1. Introduction In the dynamic world of finance, options play a crucial role in risk management, speculation, and portfolio optimization. An option is a contract that gives the holder the right, but not the obligation, to buy (call option) or sell (put option) an underlying asset at a predetermined price (strike price) within a specific time frame. The challenge lies in accurately pricing these financial instruments, given the uncertainties in market movements.","tags":["Finance","Options","Statistics"],"title":"Monte Carlo Simulation for Option Pricing"},{"categories":["Finance"],"contents":"Introduction In this article, we will explore time series data extracted from the stock market, focusing on prominent technology companies such as Apple, Amazon, Google, and Microsoft. Our objective is to equip data analysts and scientists with the essential skills to effectively manipulate and interpret stock market data.\nTo achieve this, we will utilize the yfinance library to fetch stock information and leverage visualization tools such as Seaborn and Matplotlib to illustrate various facets of the data. Specifically, we will explore methods to analyze stock risk based on historical performance, and implement predictive modeling using GRU/ LSTM models.\nThroughout this tutorial, we aim to address the following key questions:\nHow has the stock price evolved over time? What is the average daily return of the stock? How does the moving average of the stocks vary? What is the correlation between different stocks? How can we forecast future stock behavior, exemplified by predicting the closing price of Apple Inc. using LSTM or GRU?\u0026quot; Getting Data The initial step involves acquiring and loading the data into memory. Our source of stock data is the Yahoo Finance website, renowned for its wealth of financial market data and investment tools. To access this data, we\u0026rsquo;ll employ the yfinance library, known for its efficient and Pythonic approach to downloading market data from Yahoo. For further insights into yfinance, refer to the article titled Reliably download historical market data from with Python.\nInstall Dependencies pip install -qU yfinance seaborn Configuration Code import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns.set_style(\u0026#39;whitegrid\u0026#39;) plt.style.use(\u0026#34;fivethirtyeight\u0026#34;) %matplotlib inline #comment if you are not using a jupyter notebook # For reading stock data from yahoo from pandas_datareader.data import DataReader import yfinance as yf from pandas_datareader import data as pdr yf.pdr_override() # For time stamps from datetime import datetime # Get Microsoft data data = yf.download(\u0026#34;MSFT\u0026#34;, start, end) Statistical Analysis on the price Summary # Summary Stats data.describe() Closing Price The closing price is the last price at which the stock is traded during the regular trading day. A stock’s closing price is the standard benchmark used by investors to track its performance over time.\nplt.figure(figsize=(14, 5)) plt.plot(data[\u0026#39;Adj Close\u0026#39;], label=\u0026#39;Close Price\u0026#39;) plt.xlabel(\u0026#39;Date\u0026#39;) plt.ylabel(\u0026#39;Close Price [$]\u0026#39;) plt.title(\u0026#39;Stock Price History\u0026#39;) plt.legend() plt.show() Volume of Sales Volume is the amount of an asset or security that changes hands over some period of time, often over the course of a day. For instance, the stock trading volume would refer to the number of shares of security traded between its daily open and close. Trading volume, and changes to volume over the course of time, are important inputs for technical traders.\nplt.figure(figsize=(14, 5)) plt.plot(data[\u0026#39;Volume\u0026#39;], label=\u0026#39;Volume\u0026#39;) plt.xlabel(\u0026#39;Date\u0026#39;) plt.ylabel(\u0026#39;Volume\u0026#39;) plt.title(\u0026#39;Stock Price History\u0026#39;) plt.show() Moving Average The moving average (MA) is a simple technical analysis tool that smooths out price data by creating a constantly updated average price. The average is taken over a specific period of time, like 10 days, 20 minutes, 30 weeks, or any time period the trader chooses.\nma_day = [10, 20, 50] # compute moving average (can be also done in a vectorized way) for ma in ma_day: column_name = f\u0026#34;{ma} days MA\u0026#34; data[column_name] = data[\u0026#39;Adj Close\u0026#39;].rolling(ma).mean() plt.figure(figsize=(14, 5)) data[[\u0026#39;Adj Close\u0026#39;, \u0026#39;10 days MA\u0026#39;, \u0026#39;20 days MA\u0026#39;, \u0026#39;50 days MA\u0026#39;]].plot() plt.xlabel(\u0026#39;Date\u0026#39;) plt.ylabel(\u0026#39;Volume\u0026#39;) plt.title(\u0026#39;Stock Price History\u0026#39;) plt.show() Statistical Analysis on the returns Now that we\u0026rsquo;ve done some baseline analysis, let\u0026rsquo;s go ahead and dive a little deeper. We\u0026rsquo;re now going to analyze the risk of the stock. In order to do so we\u0026rsquo;ll need to take a closer look at the daily changes of the stock, and not just its absolute value. Let\u0026rsquo;s go ahead and use pandas to retrieve teh daily returns for the Microsoft stock.\n# Compute daily return in percentage data[\u0026#39;Daily Return\u0026#39;] = data[\u0026#39;Adj Close\u0026#39;].pct_change() # simple plot plt.figure(figsize=(14, 5)) data[\u0026#39;Daily Return\u0026#39;].hist(bins=50) plt.title(\u0026#39;MSFT Daily Return Distribution\u0026#39;) plt.xlabel(\u0026#39;Daily Return\u0026#39;) plt.show() # histogram plt.figure(figsize=(8, 5)) data[\u0026#39;Daily Return\u0026#39;].plot() plt.title(\u0026#39;MSFT Daily Return\u0026#39;) plt.show() Data Preparation # Create a new dataframe with only the \u0026#39;Close column X = data.filter([\u0026#39;Adj Close\u0026#39;]) # Convert the dataframe to a numpy array X = X.values # Get the number of rows to train the model on training_data_len = int(np.ceil(len(X)*.95)) # Scale data from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler(feature_range=(0,1)) scaled_data = scaler.fit_transform(X) scaled_data Split training data into small chunks to ingest into LSTM and GRU\n# Create the training data set # Create the scaled training data set train_data = scaled_data[0:int(training_data_len), :] # Split the data into x_train and y_train data sets x_train = [] y_train = [] seq_length = 60 for i in range(seq_length, len(train_data)): x_train.append(train_data[i-60:i, 0]) y_train.append(train_data[i, 0]) if i\u0026lt;= seq_length+1: print(x_train) print(y_train, end=\u0026#34;\\n\\n\u0026#34;) # Convert the x_train and y_train to numpy arrays x_train, y_train = np.array(x_train), np.array(y_train) # Reshape the data x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1)) GRU Gated-Recurrent Unit (GRU) is adopted in this part\nfrom tensorflow.keras.models import Sequential from tensorflow.keras.layers import GRU, Dense, Dropout lstm_model = Sequential() lstm_model.add(GRU(units=128, return_sequences=True, input_shape=(seq_length, 1))) lstm_model.add(Dropout(0.2)) lstm_model.add(GRU(units=64, return_sequences=False)) lstm_model.add(Dropout(0.2)) lstm_model.add(Dense(units=1)) lstm_model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;mean_squared_error\u0026#39;) lstm_model.fit(x_train, y_train, epochs=10, batch_size=8) LSTM Long Short-Term Memory (LSTM) is adopted in this part\nfrom tensorflow.keras.layers import LSTM lstm_model = Sequential() lstm_model.add(LSTM(units=128, return_sequences=True, input_shape=(seq_length, 1))) lstm_model.add(Dropout(0.2)) lstm_model.add(LSTM(units=64, return_sequences=False)) lstm_model.add(Dropout(0.2)) lstm_model.add(Dense(units=1)) lstm_model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;mean_squared_error\u0026#39;) lstm_model.fit(x_train, y_train, epochs=10, batch_size=8) Testing Metrics root mean squared error (RMSE) # Create the testing data set # Create a new array containing scaled values from index 1543 to 2002 test_data = scaled_data[training_data_len - 60: , :] # Create the data sets x_test and y_test x_test = [] y_test = dataset[training_data_len:, :] for i in range(60, len(test_data)): x_test.append(test_data[i-60:i, 0]) # Convert the data to a numpy array x_test = np.array(x_test) # Reshape the data x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 )) # Get the models predicted price values predictions_gru = gru_model.predict(x_test) predictions_gru = scaler.inverse_transform(predictions_gru) predictions_lstm = lstm_model.predict(x_test) predictions_lstm = scaler.inverse_transform(predictions_lstm) # Get the root mean squared error (RMSE) rmse_lstm = np.sqrt(np.mean(((predictions_lstm - y_test) ** 2))) rmse_gru = np.sqrt(np.mean(((predictions_gru - y_test) ** 2))) print(f\u0026#34;LSTM RMSE: {rmse_lstm:.4f}, GRU RMSE: {rmse_gru:.4f}\u0026#34;) \u0026ldquo;LSTM RMSE: 4.2341, GRU RMSE: 3.3575\u0026rdquo;\nTest Plot GRU-based model shows a bit better results both graphically and on MSE. However, this does not tell us anything about the actual profitability of these models.\nPossible trading performance The strategy implementation is:\nBUY: if prediction \u0026gt; actual_price SELL: if prediction \u0026lt; actual_price To close a position the next candle close is waited. However, LSTM and GRU has some offset that does not allow a proper utilization of this strategy.\nHence, the returns of the predictions are adopted.\n# Assume a trading capital of $10,000 trading_capital = 10000 pred_gru_df = pd.DataFrame(predictions_gru, columns=[\u0026#39;Price\u0026#39;]) pred_test_df = pd.DataFrame(y_test, columns=[\u0026#39;Price\u0026#39;]) pred_gru_df[\u0026#39;returns\u0026#39;] = pred_gru_df.pct_change(-1) pred_test_df[\u0026#39;returns\u0026#39;] = pred_test_df.pct_change(-1) # Compute Wins wins = ((pred_gru_df.dropna().returns\u0026lt;0) \u0026amp; (pred_test_df.dropna().returns\u0026lt;0)) | ((pred_gru_df.dropna().returns\u0026gt;0) \u0026amp; (pred_test_df.dropna().returns\u0026gt;0)) print(wins.value_counts()) returns_df = pd.concat([pred_gru_df.returns, pred_test_df.returns], axis=1).dropna() total_pos_return = pred_test_df.dropna().returns[wins].abs().sum() total_neg_return = pred_test_df.dropna().returns[np.logical_not(wins)].abs().sum() # compute final capital and compare with BUY\u0026amp;HOLD strategy final_capital = trading_capital*(1+total_pos_return-total_neg_return) benchmark_return = (valid.Close.iloc[-1] - valid.Close.iloc[0])/valid.Close.iloc[0] bench_capital = trading_capital*(1+benchmark_return) print(final_capital, bench_capital) returns\nTrue 81 False 72\nName: count, dtype: int64\n10535.325 9617.617\nConclusion As showed in the previous section, these two simple Deep Learning models exhibits interesting positive results both regarding regression and trading metrics. The latter is particularly important, indeed a return of 5% is obtained while the stock price decreased of approximately 4%. This also lead to a very high sharpe and colmar ratio.\n","date":"June 16, 2024","hero":"/posts/finance/stock_prediction/gru/images/stock-market-prediction-using-data-mining-techniques.jpg","permalink":"http://localhost:1313/posts/finance/stock_prediction/gru/","summary":"Introduction In this article, we will explore time series data extracted from the stock market, focusing on prominent technology companies such as Apple, Amazon, Google, and Microsoft. Our objective is to equip data analysts and scientists with the essential skills to effectively manipulate and interpret stock market data.\nTo achieve this, we will utilize the yfinance library to fetch stock information and leverage visualization tools such as Seaborn and Matplotlib to illustrate various facets of the data.","tags":["Finance","Deep Learning","Forecasting"],"title":"MSFT Stock Prediction using LSTM or GRU"},{"categories":["Physics"],"contents":"Introduction Percolation theory is a fundamental concept in statistical physics and mathematics that describes the behavior of connected clusters in a random graph. It is a model for understanding how a network behaves when nodes or links are added, leading to a phase transition from a state of disconnected clusters to a state where a large, connected cluster spans the system. This transition occurs at a critical threshold, known as the percolation threshold. The theory has applications in various fields, including material science, epidemiology, and network theory.\nWhy is Percolation Important? Useful Applications Percolation theory is important because it provides insights into the behavior of complex systems and phase transitions. Here are some key applications:\nMaterial Science: Percolation theory helps in understanding the properties of composite materials, such as conductivity and strength. For example, the electrical conductivity of a composite material can change dramatically when the concentration of conductive filler reaches the percolation threshold Epidemiology: In the study of disease spread, percolation models can predict the outbreak and spread of epidemics. The percolation threshold can represent the critical point at which a disease becomes widespread in a population Network Theory: Percolation theory is used to study the robustness and connectivity of networks, such as the internet or social networks. It helps in understanding how networks can be disrupted and how they can be made more resilient Geophysics: In oil recovery, percolation theory models the flow of fluids through porous rocks, helping to optimize extraction processes Forest Fires: Percolation models can simulate the spread of forest fires, helping in the development of strategies for fire prevention and control Mathematical and Physics Theory Percolation theory can be studied using site percolation or bond percolation models. In site percolation, each site (or node) on a lattice is either occupied with probability $ p $ or empty with probability $ 1 - p $. In bond percolation, each bond (or edge) between sites is open with probability $ p $ or closed with probability $ 1 - p $.\nStep-by-Step Explanation: Define the Lattice: Consider a 2D square lattice or a 3D cubic lattice. For simplicity, let\u0026rsquo;s use a 2D square lattice.\nAssign Probabilities: For each site (or bond), assign a probability $ p $ that it is occupied (or open).\nCluster Formation: Identify clusters of connected sites (or bonds). Two sites are in the same cluster if there is a path of occupied sites (or open bonds) connecting them.\nCritical Threshold $ p_c $: Determine the critical probability $ p_c $ at which an infinite cluster first appears. For a 2D square lattice, it has been rigorously shown that $ p_c \\approx 0.5927 $.\nMathematical Formulation: The percolation probability $ P(p) $ is the probability that a given site belongs to the infinite cluster. Near the critical threshold, this follows a power-law behavior: $$ P(p) \\sim (p - p_c)^\\beta $$ where $ \\beta $ is a critical exponent and equal to $\\frac{5}{36}$ for 2D squared lattice.\nCorrelation Length $ \\xi $: The average size of finite clusters below $ p_c $ is characterized by the correlation length $ \\xi $, which diverges as: $$ \\xi \\sim |p - p_c|^{-\\nu} $$ where $ \\nu $ is another critical exponent\nConductivity and Other Properties: In practical applications, properties like electrical conductivity in materials can be modeled by considering the effective medium theory or numerical simulations to calculate the likelihood of percolation and the size of clusters.\nBy analyzing these steps, percolation theory provides a comprehensive understanding of how macroscopic properties emerge from microscopic randomness, revealing universal behaviors that transcend specific systems.\nPython Simulation Code Here is a simple example of a site percolation simulation on a square lattice in Python:\n# -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; Created on Wed Jun 19 07:15:50 2024 @author: stefa \u0026#34;\u0026#34;\u0026#34; import numpy as np import matplotlib.pyplot as plt import scipy.ndimage def generate_lattice(n, p): \u0026#34;\u0026#34;\u0026#34;Generate an n x n lattice with site vacancy probability p.\u0026#34;\u0026#34;\u0026#34; return np.random.rand(n, n) \u0026lt; p def percolates(lattice): \u0026#34;\u0026#34;\u0026#34;Check if the lattice percolates.\u0026#34;\u0026#34;\u0026#34; intersection = get_intersection(lattice) return intersection.size \u0026gt; 1 def get_intersection(lattice): \u0026#34;\u0026#34;\u0026#34;Check if the bottom labels or top labels have more than 1 value equal \u0026#34;\u0026#34;\u0026#34; labeled_lattice, num_features = scipy.ndimage.label(lattice) top_labels = np.unique(labeled_lattice[0, :]) bottom_labels = np.unique(labeled_lattice[-1, :]) print(top_labels, bottom_labels, np.intersect1d(top_labels, bottom_labels).size) return np.intersect1d(top_labels, bottom_labels) def plot_lattice(lattice): \u0026#34;\u0026#34;\u0026#34;Plot the lattice.\u0026#34;\u0026#34;\u0026#34; plt.imshow(lattice, cmap=\u0026#39;binary\u0026#39;) plt.show() def lattice_to_image(lattice): \u0026#34;\u0026#34;\u0026#34;Convert the lattice in an RGB image (even if will be black and white)\u0026#34;\u0026#34;\u0026#34; r_ch = np.where(lattice, 0, 255) g_ch = np.where(lattice, 0, 255) b_ch = np.where(lattice, 0, 255) image = np.concatenate(( np.expand_dims(r_ch, axis=2), np.expand_dims(g_ch, axis=2), np.expand_dims(b_ch, axis=2), ), axis=2) return image def get_path(lattice): intersection = get_intersection(lattice) labeled_lattice, num_features = scipy.ndimage.label(lattice) print(intersection) return labeled_lattice == intersection[1] def add_path_img(image, path): blank_image = np.zeros(image.shape) # set the red channel to 255-\u0026gt; get a red percolation path image[path, 0] = 255 image[path, 1] = 20 return image # Parameters n = 100 # Lattice size p_values = [0.2, 0.3, 0.4, 0.5, 0.58, 0.6] # Site vacancy probabilities # Create a figure with subplots fig, axs = plt.subplots(2, 3, figsize=(15, 8)) axs = axs.flatten() # Plot lattices for different p values for i, p in enumerate(p_values): lattice = generate_lattice(n, p) lattice_img = lattice_to_image(lattice) if percolates(lattice): axs[i].set_title(f\u0026#34;p = {p} (Percolates)\u0026#34;) path = get_path(lattice) lattice_img = add_path_img(lattice_img, path) else: axs[i].set_title(f\u0026#34;p = {p} (Does not percolate)\u0026#34;) axs[i].imshow(lattice_img) axs[i].axis(\u0026#39;off\u0026#39;) # Adjust spacing between subplots plt.subplots_adjust(wspace=0.1, hspace=0.1) # Show the plot plt.show() This code generates a square lattice of size n with site vacancy probability p, checks if the lattice percolates (i.e., if there is a connected path from the top to the bottom), and plots the lattice.\nIn further version, also a connected path from left to right can be considered.\nResults Conclusion The previous plot shows that with p\u0026gt;0.58 a percolation path starts to be observed. However, this is so not alwasy happening for stochastical reasons. Hence that plot is the result of several iteration to find the most interesting plot. With p\u0026gt;0.60 percolation happens more than 90% of the time. In general, this confirms the numerical value of $p_c$ that can be found in literature of 0.5927\nIn further articles we will explore some python libraries to develop a more advanced and practical example.\n","date":"June 8, 2024","hero":"/posts/physics/percolation/images/lattice_illustration.png","permalink":"http://localhost:1313/posts/physics/percolation/","summary":"Introduction Percolation theory is a fundamental concept in statistical physics and mathematics that describes the behavior of connected clusters in a random graph. It is a model for understanding how a network behaves when nodes or links are added, leading to a phase transition from a state of disconnected clusters to a state where a large, connected cluster spans the system. This transition occurs at a critical threshold, known as the percolation threshold.","tags":["Science","Physics","Statistics"],"title":"Percolation"}]