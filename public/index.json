[{"categories":["Finance"],"contents":"Introduction In this article, we will explore time series data extracted from the stock market, focusing on prominent technology companies such as Apple, Amazon, Google, and Microsoft. Our objective is to equip data analysts and scientists with the essential skills to effectively manipulate and interpret stock market data.\nTo achieve this, we will utilize the yfinance library to fetch stock information and leverage visualization tools such as Seaborn and Matplotlib to illustrate various facets of the data. Specifically, we will explore methods to analyze stock risk based on historical performance, and implement predictive modeling using GRU/ LSTM models.\nThroughout this tutorial, we aim to address the following key questions:\nHow has the stock price evolved over time? What is the average daily return of the stock? How does the moving average of the stocks vary? What is the correlation between different stocks? How can we forecast future stock behavior, exemplified by predicting the closing price of Apple Inc. using LSTM or GRU?\u0026quot; Getting Data The initial step involves acquiring and loading the data into memory. Our source of stock data is the Yahoo Finance website, renowned for its wealth of financial market data and investment tools. To access this data, we\u0026rsquo;ll employ the yfinance library, known for its efficient and Pythonic approach to downloading market data from Yahoo. For further insights into yfinance, refer to the article titled Reliably download historical market data from with Python.\nInstall Dependencies pip install -qU yfinance seaborn Configuration Code import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns.set_style(\u0026#39;whitegrid\u0026#39;) plt.style.use(\u0026#34;fivethirtyeight\u0026#34;) %matplotlib inline #comment if you are not using a jupyter notebook # For reading stock data from yahoo from pandas_datareader.data import DataReader import yfinance as yf from pandas_datareader import data as pdr yf.pdr_override() # For time stamps from datetime import datetime # Get Microsoft data data = yf.download(\u0026#34;MSFT\u0026#34;, start, end) Statistical Analysis on the price Summary # Summary Stats data.describe() Closing Price The closing price is the last price at which the stock is traded during the regular trading day. A stockâ€™s closing price is the standard benchmark used by investors to track its performance over time.\nplt.figure(figsize=(14, 5)) plt.plot(data[\u0026#39;Adj Close\u0026#39;], label=\u0026#39;Close Price\u0026#39;) plt.xlabel(\u0026#39;Date\u0026#39;) plt.ylabel(\u0026#39;Close Price [$]\u0026#39;) plt.title(\u0026#39;Stock Price History\u0026#39;) plt.legend() plt.show() Volume of Sales Volume is the amount of an asset or security that changes hands over some period of time, often over the course of a day. For instance, the stock trading volume would refer to the number of shares of security traded between its daily open and close. Trading volume, and changes to volume over the course of time, are important inputs for technical traders.\nplt.figure(figsize=(14, 5)) plt.plot(data[\u0026#39;Volume\u0026#39;], label=\u0026#39;Volume\u0026#39;) plt.xlabel(\u0026#39;Date\u0026#39;) plt.ylabel(\u0026#39;Volume\u0026#39;) plt.title(\u0026#39;Stock Price History\u0026#39;) plt.show() Moving Average The moving average (MA) is a simple technical analysis tool that smooths out price data by creating a constantly updated average price. The average is taken over a specific period of time, like 10 days, 20 minutes, 30 weeks, or any time period the trader chooses.\nma_day = [10, 20, 50] # compute moving average (can be also done in a vectorized way) for ma in ma_day: column_name = f\u0026#34;{ma} days MA\u0026#34; data[column_name] = data[\u0026#39;Adj Close\u0026#39;].rolling(ma).mean() plt.figure(figsize=(14, 5)) data[[\u0026#39;Adj Close\u0026#39;, \u0026#39;10 days MA\u0026#39;, \u0026#39;20 days MA\u0026#39;, \u0026#39;50 days MA\u0026#39;]].plot() plt.xlabel(\u0026#39;Date\u0026#39;) plt.ylabel(\u0026#39;Volume\u0026#39;) plt.title(\u0026#39;Stock Price History\u0026#39;) plt.show() Statistical Analysis on the returns Now that we\u0026rsquo;ve done some baseline analysis, let\u0026rsquo;s go ahead and dive a little deeper. We\u0026rsquo;re now going to analyze the risk of the stock. In order to do so we\u0026rsquo;ll need to take a closer look at the daily changes of the stock, and not just its absolute value. Let\u0026rsquo;s go ahead and use pandas to retrieve teh daily returns for the Microsoft stock.\n# Compute daily return in percentage data[\u0026#39;Daily Return\u0026#39;] = data[\u0026#39;Adj Close\u0026#39;].pct_change() # simple plot plt.figure(figsize=(14, 5)) data[\u0026#39;Daily Return\u0026#39;].hist(bins=50) plt.title(\u0026#39;MSFT Daily Return Distribution\u0026#39;) plt.xlabel(\u0026#39;Daily Return\u0026#39;) plt.show() # histogram plt.figure(figsize=(8, 5)) data[\u0026#39;Daily Return\u0026#39;].plot() plt.title(\u0026#39;MSFT Daily Return\u0026#39;) plt.show() Data Preparation # Create a new dataframe with only the \u0026#39;Close column X = data.filter([\u0026#39;Adj Close\u0026#39;]) # Convert the dataframe to a numpy array X = X.values # Get the number of rows to train the model on training_data_len = int(np.ceil(len(X)*.95)) # Scale data from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler(feature_range=(0,1)) scaled_data = scaler.fit_transform(X) scaled_data Split training data into small chunks to ingest into LSTM and GRU\n# Create the training data set # Create the scaled training data set train_data = scaled_data[0:int(training_data_len), :] # Split the data into x_train and y_train data sets x_train = [] y_train = [] seq_length = 60 for i in range(seq_length, len(train_data)): x_train.append(train_data[i-60:i, 0]) y_train.append(train_data[i, 0]) if i\u0026lt;= seq_length+1: print(x_train) print(y_train, end=\u0026#34;\\n\\n\u0026#34;) # Convert the x_train and y_train to numpy arrays x_train, y_train = np.array(x_train), np.array(y_train) # Reshape the data x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1)) GRU Gated-Recurrent Unit (GRU) is adopted in this part\nfrom tensorflow.keras.models import Sequential from tensorflow.keras.layers import GRU, Dense, Dropout lstm_model = Sequential() lstm_model.add(GRU(units=128, return_sequences=True, input_shape=(seq_length, 1))) lstm_model.add(Dropout(0.2)) lstm_model.add(GRU(units=64, return_sequences=False)) lstm_model.add(Dropout(0.2)) lstm_model.add(Dense(units=1)) lstm_model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;mean_squared_error\u0026#39;) lstm_model.fit(x_train, y_train, epochs=10, batch_size=8) LSTM Long Short-Term Memory (LSTM) is adopted in this part\nfrom tensorflow.keras.layers import LSTM lstm_model = Sequential() lstm_model.add(LSTM(units=128, return_sequences=True, input_shape=(seq_length, 1))) lstm_model.add(Dropout(0.2)) lstm_model.add(LSTM(units=64, return_sequences=False)) lstm_model.add(Dropout(0.2)) lstm_model.add(Dense(units=1)) lstm_model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;mean_squared_error\u0026#39;) lstm_model.fit(x_train, y_train, epochs=10, batch_size=8) Testing Metrics root mean squared error (RMSE) # Create the testing data set # Create a new array containing scaled values from index 1543 to 2002 test_data = scaled_data[training_data_len - 60: , :] # Create the data sets x_test and y_test x_test = [] y_test = dataset[training_data_len:, :] for i in range(60, len(test_data)): x_test.append(test_data[i-60:i, 0]) # Convert the data to a numpy array x_test = np.array(x_test) # Reshape the data x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 )) # Get the models predicted price values predictions_gru = gru_model.predict(x_test) predictions_gru = scaler.inverse_transform(predictions_gru) predictions_lstm = lstm_model.predict(x_test) predictions_lstm = scaler.inverse_transform(predictions_lstm) # Get the root mean squared error (RMSE) rmse_lstm = np.sqrt(np.mean(((predictions_lstm - y_test) ** 2))) rmse_gru = np.sqrt(np.mean(((predictions_gru - y_test) ** 2))) print(f\u0026#34;LSTM RMSE: {rmse_lstm:.4f}, GRU RMSE: {rmse_gru:.4f}\u0026#34;) \u0026ldquo;LSTM RMSE: 4.2341, GRU RMSE: {3.3575}\u0026rdquo;\nTest Plot GRU-based model shows a bit better results both graphically and on MSE. However, this does not tell us anything about the actual profitability of these models.\nPossible trading performance The strategy implementation is:\nBUY: if prediction \u0026gt; actual_price SELL: if prediction \u0026lt; actual_price To close a position the next candle close is waited. However, LSTM and GRU has some offset that does not allow a proper utilization of this strategy.\nHence, the returns of the predictions are adopted.\n# Assume a trading capital of $10,000 trading_capital = 10000 pred_gru_df = pd.DataFrame(predictions_gru, columns=[\u0026#39;Price\u0026#39;]) pred_test_df = pd.DataFrame(y_test, columns=[\u0026#39;Price\u0026#39;]) pred_gru_df[\u0026#39;returns\u0026#39;] = pred_gru_df.pct_change(-1) pred_test_df[\u0026#39;returns\u0026#39;] = pred_test_df.pct_change(-1) # Compute Wins wins = ((pred_gru_df.dropna().returns\u0026lt;0) \u0026amp; (pred_test_df.dropna().returns\u0026lt;0)) | ((pred_gru_df.dropna().returns\u0026gt;0) \u0026amp; (pred_test_df.dropna().returns\u0026gt;0)) print(wins.value_counts()) returns_df = pd.concat([pred_gru_df.returns, pred_test_df.returns], axis=1).dropna() total_pos_return = pred_test_df.dropna().returns[wins].abs().sum() total_neg_return = pred_test_df.dropna().returns[np.logical_not(wins)].abs().sum() # compute final capital and compare with BUY\u0026amp;HOLD strategy final_capital = trading_capital*(1+total_pos_return-total_neg_return) benchmark_return = (valid.Close.iloc[-1] - valid.Close.iloc[0])/valid.Close.iloc[0] bench_capital = trading_capital*(1+benchmark_return) print(final_capital, bench_capital) returns True 81 False 72 Name: count, dtype: int64 10535.325897548326 9617.616876598737\nConclusion As showed in the previous section, these two simple Deep Learning models exhibits interesting positive results both regarding regression and trading metrics. The latter is particularly important, indeed a return of 5% is obtained while the stock price decreased of approximately 4%. This also lead to a very high sharpe and colmar ratio.\n","date":"June 16, 2024","hero":"/posts/finance/stock_prediction/gru/images/stock-market-prediction-using-data-mining-techniques.jpg","permalink":"http://localhost:1313/posts/finance/stock_prediction/gru/","summary":"Introduction In this article, we will explore time series data extracted from the stock market, focusing on prominent technology companies such as Apple, Amazon, Google, and Microsoft. Our objective is to equip data analysts and scientists with the essential skills to effectively manipulate and interpret stock market data.\nTo achieve this, we will utilize the yfinance library to fetch stock information and leverage visualization tools such as Seaborn and Matplotlib to illustrate various facets of the data.","tags":["Finance","Deep Learning","Forecasting"],"title":"MSFT Stock Prediction using LSTM or GRU"},{"categories":["Physics"],"contents":"Introduction Percolation theory is a fundamental concept in statistical physics and mathematics that describes the behavior of connected clusters in a random graph. It is a model for understanding how a network behaves when nodes or links are added, leading to a phase transition from a state of disconnected clusters to a state where a large, connected cluster spans the system. This transition occurs at a critical threshold, known as the percolation threshold. The theory has applications in various fields, including material science, epidemiology, and network theory.\nWhy is Percolation Important? Useful Applications Percolation theory is important because it provides insights into the behavior of complex systems and phase transitions. Here are some key applications:\nMaterial Science: Percolation theory helps in understanding the properties of composite materials, such as conductivity and strength. For example, the electrical conductivity of a composite material can change dramatically when the concentration of conductive filler reaches the percolation threshold Epidemiology: In the study of disease spread, percolation models can predict the outbreak and spread of epidemics. The percolation threshold can represent the critical point at which a disease becomes widespread in a population Network Theory: Percolation theory is used to study the robustness and connectivity of networks, such as the internet or social networks. It helps in understanding how networks can be disrupted and how they can be made more resilient Geophysics: In oil recovery, percolation theory models the flow of fluids through porous rocks, helping to optimize extraction processes Forest Fires: Percolation models can simulate the spread of forest fires, helping in the development of strategies for fire prevention and control . Python Simulation Code Here is a simple example of a site percolation simulation on a square lattice in Python:\n# -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; Created on Wed Jun 19 07:15:50 2024 @author: stefa \u0026#34;\u0026#34;\u0026#34; import numpy as np import matplotlib.pyplot as plt import scipy.ndimage def generate_lattice(n, p): \u0026#34;\u0026#34;\u0026#34;Generate an n x n lattice with site vacancy probability p.\u0026#34;\u0026#34;\u0026#34; return np.random.rand(n, n) \u0026lt; p def percolates(lattice): \u0026#34;\u0026#34;\u0026#34;Check if the lattice percolates.\u0026#34;\u0026#34;\u0026#34; intersection = get_intersection(lattice) return intersection.size \u0026gt; 1 def get_intersection(lattice): \u0026#34;\u0026#34;\u0026#34;Check if the bottom labels or top labels have more than 1 value equal \u0026#34;\u0026#34;\u0026#34; labeled_lattice, num_features = scipy.ndimage.label(lattice) top_labels = np.unique(labeled_lattice[0, :]) bottom_labels = np.unique(labeled_lattice[-1, :]) print(top_labels, bottom_labels, np.intersect1d(top_labels, bottom_labels).size) return np.intersect1d(top_labels, bottom_labels) def plot_lattice(lattice): \u0026#34;\u0026#34;\u0026#34;Plot the lattice.\u0026#34;\u0026#34;\u0026#34; plt.imshow(lattice, cmap=\u0026#39;binary\u0026#39;) plt.show() def lattice_to_image(lattice): \u0026#34;\u0026#34;\u0026#34;Convert the lattice in an RGB image (even if will be black and white)\u0026#34;\u0026#34;\u0026#34; r_ch = np.where(lattice, 0, 255) g_ch = np.where(lattice, 0, 255) b_ch = np.where(lattice, 0, 255) image = np.concatenate(( np.expand_dims(r_ch, axis=2), np.expand_dims(g_ch, axis=2), np.expand_dims(b_ch, axis=2), ), axis=2) return image def get_path(lattice): intersection = get_intersection(lattice) labeled_lattice, num_features = scipy.ndimage.label(lattice) print(intersection) return labeled_lattice == intersection[1] def add_path_img(image, path): blank_image = np.zeros(image.shape) # set the red channel to 255-\u0026gt; get a red percolation path image[path, 0] = 255 image[path, 1] = 20 return image # Parameters n = 100 # Lattice size p_values = [0.2, 0.3, 0.4, 0.5, 0.58, 0.6] # Site vacancy probabilities # Create a figure with subplots fig, axs = plt.subplots(2, 3, figsize=(15, 8)) axs = axs.flatten() # Plot lattices for different p values for i, p in enumerate(p_values): lattice = generate_lattice(n, p) lattice_img = lattice_to_image(lattice) if percolates(lattice): axs[i].set_title(f\u0026#34;p = {p} (Percolates)\u0026#34;) path = get_path(lattice) lattice_img = add_path_img(lattice_img, path) else: axs[i].set_title(f\u0026#34;p = {p} (Does not percolate)\u0026#34;) axs[i].imshow(lattice_img) axs[i].axis(\u0026#39;off\u0026#39;) # Adjust spacing between subplots plt.subplots_adjust(wspace=0.1, hspace=0.1) # Show the plot plt.show() This code generates a square lattice of size n with site vacancy probability p, checks if the lattice percolates (i.e., if there is a connected path from the top to the bottom), and plots the lattice.\nIn further version, also a connected path from left to right can be considered.\nResults Conclusion The previous plot shows that with p\u0026gt;0.58 a percolation path starts to be observed. However, this is so not alwasy happening for stochastical reasons. Hence that plot is the result of several iteration to find the most interesting plot. With p\u0026gt;0.60 percolation happens more than 90% of the time.\nIn further articles we will explore some python libraries to develop a more advanced and practical example.\n","date":"June 8, 2024","hero":"/posts/physics/percolation/images/lattice_illustration.png","permalink":"http://localhost:1313/posts/physics/percolation/","summary":"Introduction Percolation theory is a fundamental concept in statistical physics and mathematics that describes the behavior of connected clusters in a random graph. It is a model for understanding how a network behaves when nodes or links are added, leading to a phase transition from a state of disconnected clusters to a state where a large, connected cluster spans the system. This transition occurs at a critical threshold, known as the percolation threshold.","tags":["Science",""],"title":"Percolation"}]