[{"categories":["Finance"],"contents":"1. Introduction In the dynamic world of finance, options play a crucial role in risk management, speculation, and portfolio optimization. An option is a contract that gives the holder the right, but not the obligation, to buy (call option) or sell (put option) an underlying asset at a predetermined price (strike price) within a specific time frame. The challenge lies in accurately pricing these financial instruments, given the uncertainties in market movements.\nTraditional analytical methods, while powerful, often struggle with complex option structures or realistic market conditions. This is where Monte Carlo simulation steps in, offering a flexible and robust approach to option pricing. By leveraging the power of computational methods, Monte Carlo simulations can handle a wide array of option types and market scenarios, making it an indispensable tool in a quantitative analyst\u0026rsquo;s toolkit.\n2. The Black-Scholes Model Before diving into Monte Carlo methods, it\u0026rsquo;s crucial to understand the Black-Scholes model, a cornerstone in option pricing theory. Developed by Fischer Black, Myron Scholes, and Robert Merton in the early 1970s, this model revolutionized the field of quantitative finance.\nThe Black-Scholes Formula For a European call option, the Black-Scholes formula is:\n$$ C = S₀N(d_1) - Ke^{-rT}N(d_2) $$ Where: $$ d_1 = \\frac{(ln(S_0/K) + (r + σ²/2)T)}{(σ\\sqrt{T})}, \\quad d_2 = d_1 - \\sigma \\sqrt{T} $$\nC: Call option price S₀: Current stock price K: Strike price r: Risk-free interest rate T: Time to maturity σ: Volatility of the underlying asset N(x): Cumulative standard normal distribution function Assumptions of the Black-Scholes Model The Black-Scholes model rests on several key assumptions:\nThe stock price follows a geometric Brownian motion with constant drift and volatility. No arbitrage opportunities exist in the market. It\u0026rsquo;s possible to buy and sell any amount of stock or options (including fractional amounts). There are no transaction costs or taxes. All securities are perfectly divisible. The risk-free interest rate is constant and known. The underlying stock does not pay dividends. Limitations of the Black-Scholes Model While groundbreaking, the Black-Scholes model has several limitations:\nConstant Volatility: The model assumes volatility is constant, which doesn\u0026rsquo;t hold in real markets where volatility can change dramatically.\nLog-normal Distribution: It assumes stock returns are normally distributed, which doesn\u0026rsquo;t account for the fat-tailed distributions observed in reality.\nContinuous Trading: The model assumes continuous trading is possible, which isn\u0026rsquo;t realistic in practice.\nNo Dividends: It doesn\u0026rsquo;t account for dividends, which can significantly affect option prices.\nEuropean Options Only: The original model only prices European-style options, not American or exotic options.\nRisk-free Rate: It assumes a constant, known risk-free rate, which can vary in reality.\nThese limitations highlight why more flexible approaches like Monte Carlo simulation are valuable in option pricing.\n3. Monte Carlo Simulation: Theoretical Background Monte Carlo simulation addresses many of the Black-Scholes model\u0026rsquo;s limitations by using computational power to model a wide range of possible future scenarios.\nBasic Principles Monte Carlo methods use repeated random sampling to obtain numerical results. In the context of option pricing, we simulate many possible price paths for the underlying asset and then calculate the option\u0026rsquo;s payoff for each path.\nApplication to Option Pricing For option pricing, we model the stock price movement using a stochastic differential equation:\ndS = μSdt + σSdW Where:\nS: Stock price μ: Expected return σ: Volatility dW: Wiener process (random walk) This equation is then discretized for simulation purposes.\n4. Implementing Monte Carlo Simulation in Python Let\u0026rsquo;s implement a basic Monte Carlo simulation for pricing a European call option:\nimport numpy as np import matplotlib.pyplot as plt def monte_carlo_option_pricing(S0, K, T, r, sigma, num_simulations, num_steps): dt = T / num_steps paths = np.zeros((num_simulations, num_steps + 1)) paths[:, 0] = S0 for t in range(1, num_steps + 1): z = np.random.standard_normal(num_simulations) paths[:, t] = paths[:, t-1] * np.exp((r - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * z) option_payoffs = np.maximum(paths[:, -1] - K, 0) option_price = np.exp(-r * T) * np.mean(option_payoffs) return option_price, paths # Example usage S0 = 100 # Initial stock price K = 100 # Strike price T = 1 # Time to maturity (in years) r = 0.05 # Risk-free rate sigma = 0.2 # Volatility num_simulations = 10000 num_steps = 252 # Number of trading days in a year price, paths = monte_carlo_option_pricing(S0, K, T, r, sigma, num_simulations, num_steps) print(f\u0026#34;Estimated option price: {price:.2f}\u0026#34;) This code simulates multiple stock price paths, calculates the option payoff for each path, and then averages these payoffs to estimate the option price.\n5. Visualization and Analysis Visualizing the results helps in understanding the distribution of possible outcomes:\nplt.figure(figsize=(10, 6)) plt.plot(paths[:100, :].T) plt.title(\u0026#34;Sample Stock Price Paths\u0026#34;) plt.xlabel(\u0026#34;Time Steps\u0026#34;) plt.ylabel(\u0026#34;Stock Price\u0026#34;) plt.show() plt.figure(figsize=(10, 6)) plt.hist(paths[:, -1], bins=50) plt.title(\u0026#34;Distribution of Final Stock Prices\u0026#34;) plt.xlabel(\u0026#34;Stock Price\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.show() These visualizations show the range of possible stock price paths and the distribution of final stock prices, providing insight into the option\u0026rsquo;s potential outcomes.\n6. Comparison with Analytical Solutions To validate our Monte Carlo results, we can compare them with the Black-Scholes analytical solution:\nfrom scipy.stats import norm def black_scholes_call(S0, K, T, r, sigma): d1 = (np.log(S0 / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T)) d2 = d1 - sigma * np.sqrt(T) return S0 * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2) bs_price = black_scholes_call(S0, K, T, r, sigma) print(f\u0026#34;Black-Scholes price: {bs_price:.2f}\u0026#34;) print(f\u0026#34;Monte Carlo price: {price:.2f}\u0026#34;) print(f\u0026#34;Difference: {abs(bs_price - price):.4f}\u0026#34;) The difference between the two methods gives us an idea of the Monte Carlo simulation\u0026rsquo;s accuracy.\n7. Advanced Topics and Extensions Monte Carlo simulation\u0026rsquo;s flexibility allows for various extensions:\nVariance Reduction Techniques: Methods like antithetic variates can improve accuracy without increasing computational cost. Exotic Options: Monte Carlo can price complex options like Asian or barrier options, which are challenging for analytical methods. Incorporating Dividends: We can easily modify the simulation to account for dividend payments. Stochastic Volatility: Models like Heston can be implemented to account for changing volatility. 8. Conclusion Monte Carlo simulation offers a powerful and flexible approach to option pricing, addressing many limitations of analytical methods like the Black-Scholes model. While it can be computationally intensive, it handles complex scenarios and option structures with relative ease.\nThe method\u0026rsquo;s ability to incorporate various market dynamics, such as changing volatility or dividend payments, makes it invaluable in real-world financial modeling. As computational power continues to increase, Monte Carlo methods are likely to play an even more significant role in quantitative finance.\nHowever, it\u0026rsquo;s important to remember that any model, including Monte Carlo simulation, is only as good as its underlying assumptions. Careful consideration of these assumptions and regular validation against market data remain crucial in applying these techniques effectively in practice.\n","date":"June 23, 2024","hero":"/posts/finance/monte_carlo/black-scholes/Option-Pricing-Models-1.jpg","permalink":"http://localhost:1313/posts/finance/monte_carlo/black-scholes/","summary":"1. Introduction In the dynamic world of finance, options play a crucial role in risk management, speculation, and portfolio optimization. An option is a contract that gives the holder the right, but not the obligation, to buy (call option) or sell (put option) an underlying asset at a predetermined price (strike price) within a specific time frame. The challenge lies in accurately pricing these financial instruments, given the uncertainties in market movements.","tags":["Finance","Options","Statistics"],"title":"Monte Carlo Simulation for Option Pricing"},{"categories":["Finance"],"contents":"Introduction In this article, we will explore time series data extracted from the stock market, focusing on prominent technology companies such as Apple, Amazon, Google, and Microsoft. Our objective is to equip data analysts and scientists with the essential skills to effectively manipulate and interpret stock market data.\nTo achieve this, we will utilize the yfinance library to fetch stock information and leverage visualization tools such as Seaborn and Matplotlib to illustrate various facets of the data. Specifically, we will explore methods to analyze stock risk based on historical performance, and implement predictive modeling using GRU/ LSTM models.\nThroughout this tutorial, we aim to address the following key questions:\nHow has the stock price evolved over time? What is the average daily return of the stock? How does the moving average of the stocks vary? What is the correlation between different stocks? How can we forecast future stock behavior, exemplified by predicting the closing price of Apple Inc. using LSTM or GRU?\u0026quot; Getting Data The initial step involves acquiring and loading the data into memory. Our source of stock data is the Yahoo Finance website, renowned for its wealth of financial market data and investment tools. To access this data, we\u0026rsquo;ll employ the yfinance library, known for its efficient and Pythonic approach to downloading market data from Yahoo. For further insights into yfinance, refer to the article titled Reliably download historical market data from with Python.\nInstall Dependencies pip install -qU yfinance seaborn Configuration Code import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns.set_style(\u0026#39;whitegrid\u0026#39;) plt.style.use(\u0026#34;fivethirtyeight\u0026#34;) %matplotlib inline #comment if you are not using a jupyter notebook # For reading stock data from yahoo from pandas_datareader.data import DataReader import yfinance as yf from pandas_datareader import data as pdr yf.pdr_override() # For time stamps from datetime import datetime # Get Microsoft data data = yf.download(\u0026#34;MSFT\u0026#34;, start, end) Statistical Analysis on the price Summary # Summary Stats data.describe() Closing Price The closing price is the last price at which the stock is traded during the regular trading day. A stock’s closing price is the standard benchmark used by investors to track its performance over time.\nplt.figure(figsize=(14, 5)) plt.plot(data[\u0026#39;Adj Close\u0026#39;], label=\u0026#39;Close Price\u0026#39;) plt.xlabel(\u0026#39;Date\u0026#39;) plt.ylabel(\u0026#39;Close Price [$]\u0026#39;) plt.title(\u0026#39;Stock Price History\u0026#39;) plt.legend() plt.show() Volume of Sales Volume is the amount of an asset or security that changes hands over some period of time, often over the course of a day. For instance, the stock trading volume would refer to the number of shares of security traded between its daily open and close. Trading volume, and changes to volume over the course of time, are important inputs for technical traders.\nplt.figure(figsize=(14, 5)) plt.plot(data[\u0026#39;Volume\u0026#39;], label=\u0026#39;Volume\u0026#39;) plt.xlabel(\u0026#39;Date\u0026#39;) plt.ylabel(\u0026#39;Volume\u0026#39;) plt.title(\u0026#39;Stock Price History\u0026#39;) plt.show() Moving Average The moving average (MA) is a simple technical analysis tool that smooths out price data by creating a constantly updated average price. The average is taken over a specific period of time, like 10 days, 20 minutes, 30 weeks, or any time period the trader chooses.\nma_day = [10, 20, 50] # compute moving average (can be also done in a vectorized way) for ma in ma_day: column_name = f\u0026#34;{ma} days MA\u0026#34; data[column_name] = data[\u0026#39;Adj Close\u0026#39;].rolling(ma).mean() plt.figure(figsize=(14, 5)) data[[\u0026#39;Adj Close\u0026#39;, \u0026#39;10 days MA\u0026#39;, \u0026#39;20 days MA\u0026#39;, \u0026#39;50 days MA\u0026#39;]].plot() plt.xlabel(\u0026#39;Date\u0026#39;) plt.ylabel(\u0026#39;Volume\u0026#39;) plt.title(\u0026#39;Stock Price History\u0026#39;) plt.show() Statistical Analysis on the returns Now that we\u0026rsquo;ve done some baseline analysis, let\u0026rsquo;s go ahead and dive a little deeper. We\u0026rsquo;re now going to analyze the risk of the stock. In order to do so we\u0026rsquo;ll need to take a closer look at the daily changes of the stock, and not just its absolute value. Let\u0026rsquo;s go ahead and use pandas to retrieve teh daily returns for the Microsoft stock.\n# Compute daily return in percentage data[\u0026#39;Daily Return\u0026#39;] = data[\u0026#39;Adj Close\u0026#39;].pct_change() # simple plot plt.figure(figsize=(14, 5)) data[\u0026#39;Daily Return\u0026#39;].hist(bins=50) plt.title(\u0026#39;MSFT Daily Return Distribution\u0026#39;) plt.xlabel(\u0026#39;Daily Return\u0026#39;) plt.show() # histogram plt.figure(figsize=(8, 5)) data[\u0026#39;Daily Return\u0026#39;].plot() plt.title(\u0026#39;MSFT Daily Return\u0026#39;) plt.show() Data Preparation # Create a new dataframe with only the \u0026#39;Close column X = data.filter([\u0026#39;Adj Close\u0026#39;]) # Convert the dataframe to a numpy array X = X.values # Get the number of rows to train the model on training_data_len = int(np.ceil(len(X)*.95)) # Scale data from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler(feature_range=(0,1)) scaled_data = scaler.fit_transform(X) scaled_data Split training data into small chunks to ingest into LSTM and GRU\n# Create the training data set # Create the scaled training data set train_data = scaled_data[0:int(training_data_len), :] # Split the data into x_train and y_train data sets x_train = [] y_train = [] seq_length = 60 for i in range(seq_length, len(train_data)): x_train.append(train_data[i-60:i, 0]) y_train.append(train_data[i, 0]) if i\u0026lt;= seq_length+1: print(x_train) print(y_train, end=\u0026#34;\\n\\n\u0026#34;) # Convert the x_train and y_train to numpy arrays x_train, y_train = np.array(x_train), np.array(y_train) # Reshape the data x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1)) GRU Gated-Recurrent Unit (GRU) is adopted in this part\nfrom tensorflow.keras.models import Sequential from tensorflow.keras.layers import GRU, Dense, Dropout lstm_model = Sequential() lstm_model.add(GRU(units=128, return_sequences=True, input_shape=(seq_length, 1))) lstm_model.add(Dropout(0.2)) lstm_model.add(GRU(units=64, return_sequences=False)) lstm_model.add(Dropout(0.2)) lstm_model.add(Dense(units=1)) lstm_model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;mean_squared_error\u0026#39;) lstm_model.fit(x_train, y_train, epochs=10, batch_size=8) LSTM Long Short-Term Memory (LSTM) is adopted in this part\nfrom tensorflow.keras.layers import LSTM lstm_model = Sequential() lstm_model.add(LSTM(units=128, return_sequences=True, input_shape=(seq_length, 1))) lstm_model.add(Dropout(0.2)) lstm_model.add(LSTM(units=64, return_sequences=False)) lstm_model.add(Dropout(0.2)) lstm_model.add(Dense(units=1)) lstm_model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;mean_squared_error\u0026#39;) lstm_model.fit(x_train, y_train, epochs=10, batch_size=8) Testing Metrics root mean squared error (RMSE) # Create the testing data set # Create a new array containing scaled values from index 1543 to 2002 test_data = scaled_data[training_data_len - 60: , :] # Create the data sets x_test and y_test x_test = [] y_test = dataset[training_data_len:, :] for i in range(60, len(test_data)): x_test.append(test_data[i-60:i, 0]) # Convert the data to a numpy array x_test = np.array(x_test) # Reshape the data x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 )) # Get the models predicted price values predictions_gru = gru_model.predict(x_test) predictions_gru = scaler.inverse_transform(predictions_gru) predictions_lstm = lstm_model.predict(x_test) predictions_lstm = scaler.inverse_transform(predictions_lstm) # Get the root mean squared error (RMSE) rmse_lstm = np.sqrt(np.mean(((predictions_lstm - y_test) ** 2))) rmse_gru = np.sqrt(np.mean(((predictions_gru - y_test) ** 2))) print(f\u0026#34;LSTM RMSE: {rmse_lstm:.4f}, GRU RMSE: {rmse_gru:.4f}\u0026#34;) \u0026ldquo;LSTM RMSE: 4.2341, GRU RMSE: 3.3575\u0026rdquo;\nTest Plot GRU-based model shows a bit better results both graphically and on MSE. However, this does not tell us anything about the actual profitability of these models.\nPossible trading performance The strategy implementation is:\nBUY: if prediction \u0026gt; actual_price SELL: if prediction \u0026lt; actual_price To close a position the next candle close is waited. However, LSTM and GRU has some offset that does not allow a proper utilization of this strategy.\nHence, the returns of the predictions are adopted.\n# Assume a trading capital of $10,000 trading_capital = 10000 pred_gru_df = pd.DataFrame(predictions_gru, columns=[\u0026#39;Price\u0026#39;]) pred_test_df = pd.DataFrame(y_test, columns=[\u0026#39;Price\u0026#39;]) pred_gru_df[\u0026#39;returns\u0026#39;] = pred_gru_df.pct_change(-1) pred_test_df[\u0026#39;returns\u0026#39;] = pred_test_df.pct_change(-1) # Compute Wins wins = ((pred_gru_df.dropna().returns\u0026lt;0) \u0026amp; (pred_test_df.dropna().returns\u0026lt;0)) | ((pred_gru_df.dropna().returns\u0026gt;0) \u0026amp; (pred_test_df.dropna().returns\u0026gt;0)) print(wins.value_counts()) returns_df = pd.concat([pred_gru_df.returns, pred_test_df.returns], axis=1).dropna() total_pos_return = pred_test_df.dropna().returns[wins].abs().sum() total_neg_return = pred_test_df.dropna().returns[np.logical_not(wins)].abs().sum() # compute final capital and compare with BUY\u0026amp;HOLD strategy final_capital = trading_capital*(1+total_pos_return-total_neg_return) benchmark_return = (valid.Close.iloc[-1] - valid.Close.iloc[0])/valid.Close.iloc[0] bench_capital = trading_capital*(1+benchmark_return) print(final_capital, bench_capital) returns\nTrue 81 False 72\nName: count, dtype: int64\n10535.325 9617.617\nConclusion As showed in the previous section, these two simple Deep Learning models exhibits interesting positive results both regarding regression and trading metrics. The latter is particularly important, indeed a return of 5% is obtained while the stock price decreased of approximately 4%. This also lead to a very high sharpe and colmar ratio.\n","date":"June 16, 2024","hero":"/posts/finance/stock_prediction/gru/images/stock-market-prediction-using-data-mining-techniques.jpg","permalink":"http://localhost:1313/posts/finance/stock_prediction/gru/","summary":"Introduction In this article, we will explore time series data extracted from the stock market, focusing on prominent technology companies such as Apple, Amazon, Google, and Microsoft. Our objective is to equip data analysts and scientists with the essential skills to effectively manipulate and interpret stock market data.\nTo achieve this, we will utilize the yfinance library to fetch stock information and leverage visualization tools such as Seaborn and Matplotlib to illustrate various facets of the data.","tags":["Finance","Deep Learning","Forecasting"],"title":"MSFT Stock Prediction using LSTM or GRU"},{"categories":["Physics"],"contents":"Introduction Percolation theory is a fundamental concept in statistical physics and mathematics that describes the behavior of connected clusters in a random graph. It is a model for understanding how a network behaves when nodes or links are added, leading to a phase transition from a state of disconnected clusters to a state where a large, connected cluster spans the system. This transition occurs at a critical threshold, known as the percolation threshold. The theory has applications in various fields, including material science, epidemiology, and network theory.\nWhy is Percolation Important? Useful Applications Percolation theory is important because it provides insights into the behavior of complex systems and phase transitions. Here are some key applications:\nMaterial Science: Percolation theory helps in understanding the properties of composite materials, such as conductivity and strength. For example, the electrical conductivity of a composite material can change dramatically when the concentration of conductive filler reaches the percolation threshold Epidemiology: In the study of disease spread, percolation models can predict the outbreak and spread of epidemics. The percolation threshold can represent the critical point at which a disease becomes widespread in a population Network Theory: Percolation theory is used to study the robustness and connectivity of networks, such as the internet or social networks. It helps in understanding how networks can be disrupted and how they can be made more resilient Geophysics: In oil recovery, percolation theory models the flow of fluids through porous rocks, helping to optimize extraction processes Forest Fires: Percolation models can simulate the spread of forest fires, helping in the development of strategies for fire prevention and control Mathematical and Physics Theory Percolation theory can be studied using site percolation or bond percolation models. In site percolation, each site (or node) on a lattice is either occupied with probability $ p $ or empty with probability $ 1 - p $. In bond percolation, each bond (or edge) between sites is open with probability $ p $ or closed with probability $ 1 - p $.\nStep-by-Step Explanation: Define the Lattice: Consider a 2D square lattice or a 3D cubic lattice. For simplicity, let\u0026rsquo;s use a 2D square lattice.\nAssign Probabilities: For each site (or bond), assign a probability $ p $ that it is occupied (or open).\nCluster Formation: Identify clusters of connected sites (or bonds). Two sites are in the same cluster if there is a path of occupied sites (or open bonds) connecting them.\nCritical Threshold $ p_c $: Determine the critical probability $ p_c $ at which an infinite cluster first appears. For a 2D square lattice, it has been rigorously shown that $ p_c \\approx 0.5927 $.\nMathematical Formulation: The percolation probability $ P(p) $ is the probability that a given site belongs to the infinite cluster. Near the critical threshold, this follows a power-law behavior: $$ P(p) \\sim (p - p_c)^\\beta $$ where $ \\beta $ is a critical exponent and equal to $\\frac{5}{36}$ for 2D squared lattice.\nCorrelation Length $ \\xi $: The average size of finite clusters below $ p_c $ is characterized by the correlation length $ \\xi $, which diverges as: $$ \\xi \\sim |p - p_c|^{-\\nu} $$ where $ \\nu $ is another critical exponent\nConductivity and Other Properties: In practical applications, properties like electrical conductivity in materials can be modeled by considering the effective medium theory or numerical simulations to calculate the likelihood of percolation and the size of clusters.\nBy analyzing these steps, percolation theory provides a comprehensive understanding of how macroscopic properties emerge from microscopic randomness, revealing universal behaviors that transcend specific systems.\nPython Simulation Code Here is a simple example of a site percolation simulation on a square lattice in Python:\n# -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; Created on Wed Jun 19 07:15:50 2024 @author: stefa \u0026#34;\u0026#34;\u0026#34; import numpy as np import matplotlib.pyplot as plt import scipy.ndimage def generate_lattice(n, p): \u0026#34;\u0026#34;\u0026#34;Generate an n x n lattice with site vacancy probability p.\u0026#34;\u0026#34;\u0026#34; return np.random.rand(n, n) \u0026lt; p def percolates(lattice): \u0026#34;\u0026#34;\u0026#34;Check if the lattice percolates.\u0026#34;\u0026#34;\u0026#34; intersection = get_intersection(lattice) return intersection.size \u0026gt; 1 def get_intersection(lattice): \u0026#34;\u0026#34;\u0026#34;Check if the bottom labels or top labels have more than 1 value equal \u0026#34;\u0026#34;\u0026#34; labeled_lattice, num_features = scipy.ndimage.label(lattice) top_labels = np.unique(labeled_lattice[0, :]) bottom_labels = np.unique(labeled_lattice[-1, :]) print(top_labels, bottom_labels, np.intersect1d(top_labels, bottom_labels).size) return np.intersect1d(top_labels, bottom_labels) def plot_lattice(lattice): \u0026#34;\u0026#34;\u0026#34;Plot the lattice.\u0026#34;\u0026#34;\u0026#34; plt.imshow(lattice, cmap=\u0026#39;binary\u0026#39;) plt.show() def lattice_to_image(lattice): \u0026#34;\u0026#34;\u0026#34;Convert the lattice in an RGB image (even if will be black and white)\u0026#34;\u0026#34;\u0026#34; r_ch = np.where(lattice, 0, 255) g_ch = np.where(lattice, 0, 255) b_ch = np.where(lattice, 0, 255) image = np.concatenate(( np.expand_dims(r_ch, axis=2), np.expand_dims(g_ch, axis=2), np.expand_dims(b_ch, axis=2), ), axis=2) return image def get_path(lattice): intersection = get_intersection(lattice) labeled_lattice, num_features = scipy.ndimage.label(lattice) print(intersection) return labeled_lattice == intersection[1] def add_path_img(image, path): blank_image = np.zeros(image.shape) # set the red channel to 255-\u0026gt; get a red percolation path image[path, 0] = 255 image[path, 1] = 20 return image # Parameters n = 100 # Lattice size p_values = [0.2, 0.3, 0.4, 0.5, 0.58, 0.6] # Site vacancy probabilities # Create a figure with subplots fig, axs = plt.subplots(2, 3, figsize=(15, 8)) axs = axs.flatten() # Plot lattices for different p values for i, p in enumerate(p_values): lattice = generate_lattice(n, p) lattice_img = lattice_to_image(lattice) if percolates(lattice): axs[i].set_title(f\u0026#34;p = {p} (Percolates)\u0026#34;) path = get_path(lattice) lattice_img = add_path_img(lattice_img, path) else: axs[i].set_title(f\u0026#34;p = {p} (Does not percolate)\u0026#34;) axs[i].imshow(lattice_img) axs[i].axis(\u0026#39;off\u0026#39;) # Adjust spacing between subplots plt.subplots_adjust(wspace=0.1, hspace=0.1) # Show the plot plt.show() This code generates a square lattice of size n with site vacancy probability p, checks if the lattice percolates (i.e., if there is a connected path from the top to the bottom), and plots the lattice.\nIn further version, also a connected path from left to right can be considered.\nResults Conclusion The previous plot shows that with p\u0026gt;0.58 a percolation path starts to be observed. However, this is so not alwasy happening for stochastical reasons. Hence that plot is the result of several iteration to find the most interesting plot. With p\u0026gt;0.60 percolation happens more than 90% of the time. In general, this confirms the numerical value of $p_c$ that can be found in literature of 0.5927\nIn further articles we will explore some python libraries to develop a more advanced and practical example.\n","date":"June 8, 2024","hero":"/posts/physics/percolation/images/lattice_illustration.png","permalink":"http://localhost:1313/posts/physics/percolation/","summary":"Introduction Percolation theory is a fundamental concept in statistical physics and mathematics that describes the behavior of connected clusters in a random graph. It is a model for understanding how a network behaves when nodes or links are added, leading to a phase transition from a state of disconnected clusters to a state where a large, connected cluster spans the system. This transition occurs at a critical threshold, known as the percolation threshold.","tags":["Science","Physics","Statistics"],"title":"Percolation"}]