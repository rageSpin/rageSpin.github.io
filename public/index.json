[{"categories":["Finance"],"contents":"1. Introduction Time series analysis is a fundamental technique in quantitative finance, particularly for understanding and predicting stock price movements. Among the various time series models, ARIMA (Autoregressive Integrated Moving Average) models have gained popularity due to their flexibility and effectiveness in capturing complex patterns in financial data.\nThis article will explore the application of time series analysis and ARIMA models to stock price prediction. We\u0026rsquo;ll cover the theoretical foundations, practical implementation in Python, and critical considerations for using these models in real-world financial scenarios.\n2. Fundamentals of Time Series Analysis Components of a Time Series A time series typically consists of four components:\nTrend: The long-term movement in the series Seasonality: Regular, periodic fluctuations Cyclical: Irregular fluctuations, often related to economic cycles Residual: Random, unpredictable variations Understanding these components is crucial for effective time series modeling.\nStationarity A key concept in time series analysis is stationarity. A stationary time series has constant statistical properties over time, including mean and variance. Many time series models, including ARIMA, assume stationarity. We often need to transform non-stationary data (like most stock price series) to achieve stationarity. Augmented Dickey-Fuller test can be used to check for stationarity, as showed in next sections.\n3. ARIMA Models: Theoretical Background ARIMA models combine three components:\nAR (Autoregressive): The model uses the dependent relationship between an observation and some number of lagged observations. I (Integrated): The use of differencing of raw observations to make the time series stationary. MA (Moving Average): The model uses the dependency between an observation and a residual error from a moving average model applied to lagged observations. The ARIMA model is typically denoted as ARIMA(p,d,q), where:\np is the order of the AR term d is the degree of differencing q is the order of the MA term Mathematical Representation The ARIMA model can be written as:\n$$ Y_t = c + \\varphi_1 Y_{t-1} + \\varphi_2 Y_{t-2} + \u0026hellip; + \\varphi_p Y_{t-p} + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \u0026hellip; + \\theta_q \\epsilon_{t-q} + \\epsilon_t $$\nWhere:\n$Y_t$ is the differenced series (it may have been differenced more than once) c is a constant $\\phi_i$ are the parameters of the autoregressive part $\\theta_i$ are the parameters of the moving average part $\\epsilon_t$ is white noise 4. Implementing ARIMA Models in Python Let\u0026rsquo;s implement an ARIMA model for stock price prediction using Python and the statsmodels library:\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt from statsmodels.tsa.arima.model import ARIMA from statsmodels.tsa.stattools import adfuller from sklearn.metrics import mean_squared_error import yfinance as yf import seaborn as sns # Download stock data ticker = \u0026#34;AAPL\u0026#34; start_date = \u0026#34;2018-01-01\u0026#34; end_date = \u0026#34;2024-06-23\u0026#34; data = yf.download(ticker, start=start_date, end=end_date) # Prepare the data ts = data[\u0026#39;Close\u0026#39;] # Check for stationarity def test_stationarity(timeseries): result = adfuller(timeseries, autolag=\u0026#39;AIC\u0026#39;) print(\u0026#39;ADF Statistic:\u0026#39;, result[0]) print(\u0026#39;p-value:\u0026#39;, result[1]) return result[1] # Plot the time-series plt.figure(figsize=(12,6)) plt.plot(ts.index[:], ts.values[:], label=\u0026#39;Observed\u0026#39;) plt.title(f\u0026#39;{ticker} Stock Price \u0026#39;) # plt.legend() plt.tight_layout() plt.show() p_val = test_stationarity(ts) if p_val \u0026gt; 0.05: # If non-stationary, difference the series ts_diff = ts.diff().dropna() p_val = test_stationarity(ts_diff) d = 1 if p_val \u0026gt; 0.05: ts_diff = ts.diff().diff().dropna() p_val = test_stationarity(ts_diff) d = 2 print(f\u0026#34;\\nd = {d}\u0026#34;) Output:\nd = 1\nThis script downloads stock data, checks for stationarity, fits an ARIMA model, makes predictions, and evaluates the model\u0026rsquo;s performance. In this case, as expected from the plot, the time-series is not stationary. Hence, d has to be greater or equal to 1.\n5. Model Selection and Diagnostic Checking Choosing the right ARIMA model involves selecting appropriate values for p, d, and q. This process often involves:\nAnalyzing ACF and PACF plots: These help in identifying potential AR and MA orders. Grid search: Trying different combinations of p, d, and q and selecting the best based on information criteria like AIC or BIC. Diagnostic checking: Analyzing residuals to ensure they resemble white noise. Finding ARIMA Parameters (p, d, q) Determining the optimal ARIMA parameters involves a combination of statistical tests, visual inspection, and iterative processes. Here\u0026rsquo;s a systematic approach to finding p, d, and q:\nDetermine d (Differencing Order): Use the Augmented Dickey-Fuller test to check for stationarity. If the series is not stationary, difference it and test again until stationarity is achieved. Determine p (AR Order) and q (MA Order): After differencing, use ACF (Autocorrelation Function) and PACF (Partial Autocorrelation Function) plots. The lag where the ACF cuts off indicates the q value. The lag where the PACF cuts off indicates the p value. Fine-tune with Information Criteria: Use AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) to compare different models. Finding d parameter from plots Since, the stationary was already checkd in the previous, this paragraph is useful for graphical and comphrension purpose. Moreover, with autocorrelation parameters, it is possible to find better values of d that the ADF test cannot recognize.\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf plt.rcParams.update({\u0026#39;figure.figsize\u0026#39;:(15,10), \u0026#39;figure.dpi\u0026#39;:80}) # Import data df = data.copy() # Original Series fig, axes = plt.subplots(3, 2, sharex=False) axes[0, 0].plot(df.index, df.Close); axes[0, 0].set_title(\u0026#39;Original Series - \u0026#39;+ticker) plot_acf(df.Close, ax=axes[0, 1], lags=len(df)-1, color=\u0026#39;k\u0026#39;, auto_ylims=True) # 1st Differencing axes[1, 0].plot(df.index, df.Close.diff()); axes[1, 0].set_title(\u0026#39;1st Order Differencing\u0026#39;) plot_acf(df.Close.diff().dropna(), ax=axes[1, 1], lags=len(df)/7-2, color=\u0026#39;k\u0026#39;, auto_ylims=True) # 2nd Differencing axes[2, 0].plot(df.index, df.Close.diff().diff()); axes[2, 0].set_title(\u0026#39;2nd Order Differencing\u0026#39;) plot_acf(df.Close.diff().diff().dropna(), ax=axes[2, 1], lags=len(df)/7-3, color=\u0026#39;k\u0026#39;, auto_ylims=True) plt.tight_layout() plt.show() Indeed, from the plot, d=2 is probably a better solution since we have few coefficient that goes above the confidence threshold.\nFinding p parameter from plots As suggest previously, Partical Correlation Plot is adopted to find the p parameter.\nplt.rcParams.update({\u0026#39;figure.figsize\u0026#39;:(15,5), \u0026#39;figure.dpi\u0026#39;:80}) fig, axes = plt.subplots(1, 2, sharex=False) axes[0].plot(df.index, df.Close.diff()); axes[0].set_title(\u0026#39;1st Differencing\u0026#39;) axes[1].set(ylim=(0,5)) plot_pacf(df.Close.diff().dropna(), ax=axes[1], lags=200, color=\u0026#39;k\u0026#39;, auto_ylims=True, zero=False) plt.tight_layout() plt.show() A possible choice of p can 8 or 18, where the coefficient crosses the confidence intervals.\nFinding q parameter from plots plt.rcParams.update({\u0026#39;figure.figsize\u0026#39;:(15,5), \u0026#39;figure.dpi\u0026#39;:80}) fig, axes = plt.subplots(1, 2, sharex=False) axes[0].plot(df.Close.diff()); axes[0].set_title(\u0026#39;1st Differencing\u0026#39;) axes[1].set(ylim=(0,1.2)) plot_acf(df.Close.diff().dropna(), ax=axes[1], lags=200, color=\u0026#39;k\u0026#39;, auto_ylims=True, zero=False) plt.tight_layout() plt.show() ACF looks very similar to PCF for smaller lags. Hence, even in this case a value of 8 can be used as q.\n6. ARIMA model fitting Predict ARIMA model on all data model = ARIMA(df.Close, order=(8,2,8)) # p,d,q results = model.fit() print(results.summary()) # Actual vs Fitted o plt.plot(results.predict()[-100:], \u0026#39;-*\u0026#39;, label=\u0026#39;prediction\u0026#39;) plt.plot(df.Close[-100:], \u0026#39;-*\u0026#39;, label=\u0026#39;actual\u0026#39;) plt.legend() plt.title(\u0026#34;Prediction vs. Actual on All Data \u0026#34;) plt.tight_layout() plt.show() Train/ Test split from statsmodels.tsa.stattools import acf # Create Training and Test train = df.Close[:int(len(df)*0.8)] test = df.Close[int(len(df)*0.8):] # model = ARIMA(train, order=(3,2,1)) model = ARIMA(train, order=(8, 2, 8)) fitted = model.fit() # Forecast fc = fitted.get_forecast(steps=len(test), alpha=0.05) # 95% conf conf = fc.conf_int() # Make as pandas series fc_series = pd.Series(fitted.forecast(steps=len(test)).values, index=test.index) lower_series = pd.Series(conf.iloc[:, 0].values, index=test.index) upper_series = pd.Series(conf.iloc[:, 1].values, index=test.index) # Plot plt.figure(figsize=(12,5), dpi=100) plt.plot(train[-200:], label=\u0026#39;training\u0026#39;) plt.plot(test, label=\u0026#39;actual\u0026#39;) plt.plot(fc_series, label=\u0026#39;forecast\u0026#39;) plt.fill_between(lower_series.index, lower_series, upper_series, color=\u0026#39;k\u0026#39;, alpha=.15) plt.title(\u0026#39;Forecast vs Actuals\u0026#39;) plt.legend(loc=\u0026#39;upper left\u0026#39;, fontsize=10) plt.tight_layout() plt.show() # Accuracy metrics def forecast_accuracy(forecast, actual): mape = np.mean(np.abs(forecast - actual)/np.abs(actual)) # MAPE me = np.mean(forecast - actual) # ME mae = np.mean(np.abs(forecast - actual)) # MAE mpe = np.mean((forecast - actual)/actual) # MPE rmse = np.mean((forecast - actual)**2)**.5 # RMSE corr = np.corrcoef(forecast, actual)[0,1] # corr mins = np.amin(np.hstack([forecast[:,None], actual[:,None]]), axis=1) maxs = np.amax(np.hstack([forecast[:,None], actual[:,None]]), axis=1) minmax = 1 - np.mean(mins/maxs) # minmax acf1 = acf(forecast-test)[1] # ACF1 return({\u0026#39;mape\u0026#39;:mape, \u0026#39;me\u0026#39;:me, \u0026#39;mae\u0026#39;: mae, \u0026#39;mpe\u0026#39;: mpe, \u0026#39;rmse\u0026#39;:rmse, \u0026#39;acf1\u0026#39;:acf1, \u0026#39;corr\u0026#39;:corr, \u0026#39;minmax\u0026#39;:minmax}) forecast_accuracy(fc_series.values, test.values) Output:\n{\u0026lsquo;mape\u0026rsquo;: 0.07829701788549515,\n\u0026lsquo;me\u0026rsquo;: -12.898037657120996,\n\u0026lsquo;mae\u0026rsquo;: 14.483068468837455,\n\u0026lsquo;mpe\u0026rsquo;: -0.068860507560246,\n\u0026lsquo;rmse\u0026rsquo;: 16.906382957008496,\n\u0026lsquo;acf1\u0026rsquo;: 0.9702976318229376,\n\u0026lsquo;corr\u0026rsquo;: 0.4484875181364141,\n\u0026lsquo;minmax\u0026rsquo;: 0.07810488835602647}\nGrid Search def grid_search_arima(train, test, p_range, d_range, q_range): best_aic = float(\u0026#39;inf\u0026#39;) best_mape = float(\u0026#39;inf\u0026#39;) best_order = None for p in p_range: for d in d_range: for q in q_range: try: model = ARIMA(train.values, order=(p,d,q)) results = model.fit() fc_series = pd.Series(results.forecast(steps=len(test)), index=test.index) # 95% conf test_metrics = forecast_accuracy(fc_series.values, test.values) # if results.aic \u0026lt; best_aic: # best_aic = results.aic # best_order = (p,d,q) print(p,d,q, test_metrics[\u0026#39;mape\u0026#39;]) if test_metrics[\u0026#39;mape\u0026#39;] \u0026lt; best_mape: best_mape = test_metrics[\u0026#39;mape\u0026#39;] best_order = (p,d,q) print(\u0026#34;temp best:\u0026#34;, best_order, test_metrics[\u0026#39;mape\u0026#39;]) except Exception as e: print(e) continue return best_order # Grid search for best p and q (assuming d is known) best_order = grid_search_arima(train, test, range(1,9), [d, d+1], range(1,9)) print(f\u0026#34;Best ARIMA order based on grid search: {best_order}\u0026#34;) Suggested d value: 1\ntemp best: (1, 1, 1) 0.14570196898952395\ntemp best: (1, 1, 5) 0.14514639508226412\ntemp best: (1, 1, 6) 0.14499024417142595\ntemp best: (1, 1, 7) 0.1439625731680348\ntemp best: (1, 2, 1) 0.07729490750827837\ntemp best: (1, 2, 2) 0.0764917667521908\ntemp best: (3, 2, 4) 0.07647187068962996\nBest ARIMA order based on grid search: (3, 2, 4)\nAt the end, we found that (3,2,4) offer better testing results (mape of 7.6% vs. 7.8% of manual parameters finding), even if there is only a tiny difference (0.2%). Moreover, since the values of p and q are lower the model will be faster and less prone to overfitting (smaller values of AR or MA coefficients).\n7. Limitations and Considerations While ARIMA models can be powerful for time series prediction, they have limitations:\nAssumption of linearity: ARIMA models assume linear relationships, which may not hold for complex financial data. Limited forecasting horizon: They tend to perform poorly for long-term forecasts. Sensitivity to outliers: Extreme values can significantly impact model performance. Assumption of constant variance: This may not hold for volatile stock prices. No consideration of external factors: ARIMA models only use past values of the time series, ignoring other potentially relevant variables. 8. Conclusion Time series analysis and ARIMA models provide valuable tools for understanding and predicting stock price movements. While they have limitations, particularly in the complex and often non-linear world of financial markets, they serve as a strong foundation for more advanced modeling techniques.\nWhen applying these models to real-world financial data, it\u0026rsquo;s crucial to:\nThoroughly understand the underlying assumptions Carefully preprocess and analyze the data Conduct rigorous model selection and diagnostic checking Interpret results with caution, considering the model\u0026rsquo;s limitations Combine with other analytical techniques and domain expertise for comprehensive analysis As with all financial modeling, remember that past performance does not guarantee future results. Time series models should be one tool in a broader analytical toolkit, complemented by fundamental analysis, market sentiment assessment, and a deep understanding of the specific stock and its market context.\nNext Steps In next articles, we are going to explore about time-series decomposition, seasanality, exogenous variables. Indeed, several extensions to basic ARIMA models address some of these limitations:\nSARIMA: Incorporates seasonality. ARIMAX: Includes exogenous variables. GARCH: Models time-varying volatility. Vector ARIMA: Handles multiple related time series simultaneously. ","date":"June 28, 2024","hero":"/posts/finance/stock_prediction/arima/images/test_forecast.png","permalink":"http://localhost:1313/posts/finance/stock_prediction/arima/","summary":"1. Introduction Time series analysis is a fundamental technique in quantitative finance, particularly for understanding and predicting stock price movements. Among the various time series models, ARIMA (Autoregressive Integrated Moving Average) models have gained popularity due to their flexibility and effectiveness in capturing complex patterns in financial data.\nThis article will explore the application of time series analysis and ARIMA models to stock price prediction. We\u0026rsquo;ll cover the theoretical foundations, practical implementation in Python, and critical considerations for using these models in real-world financial scenarios.","tags":["Finance","Statistics","Forecasting"],"title":"Time Series Analysis and ARIMA Models for Stock Price Prediction"},{"categories":["Computer Vision"],"contents":"Install dependencies Type the following command to install possible needed dependencies (especially if the inference is performed on the CPU)\n%pip install einops flash_attn In Kaggle, transformers and torch are already installed. Otherwise you also need to install them on your local PC.\nImport Libraries from transformers import AutoProcessor, AutoModelForCausalLM from PIL import Image import requests import copy import torch %matplotlib inline Import the model We can choose Florence-2-large or Florence-2-large-ft (fine-tuned).\nmodel_id = \u0026#39;microsoft/Florence-2-large-ft\u0026#39; device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) print(device) model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).eval() model = model.to(device) # put the model on the available GPU processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) Define inference function def run_inference(task_prompt, text_input=None): if text_input is None: prompt = task_prompt else: prompt = task_prompt + text_input inputs = processor(text=prompt, images=image, return_tensors=\u0026#34;pt\u0026#34;).to(device) generated_ids = model.generate( input_ids=inputs[\u0026#34;input_ids\u0026#34;], pixel_values=inputs[\u0026#34;pixel_values\u0026#34;], max_new_tokens=1024, early_stopping=False, do_sample=False, num_beams=3, ) generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0] parsed_answer = processor.post_process_generation( generated_text, task=task_prompt, image_size=(image.width, image.height) ) return parsed_answer Get image link image_url = \u0026#34;http://lerablog.org/wp-content/uploads/2013/06/two-cars.jpg\u0026#34; # an arbitrary image link or filepath can be inserted here image = Image.open(requests.get(image_url, stream=True).raw) image Run pre-defined tasks without additional inputs Caption task_prompt = \u0026#39;\u0026lt;CAPTION\u0026gt;\u0026#39; run_inference(task_prompt) {\u0026rsquo;\u0026rsquo;: \u0026lsquo;Two sports cars parked next to each other on a road.\u0026rsquo;}\ntask_prompt = \u0026#39;\u0026lt;DETAILED_CAPTION\u0026gt;\u0026#39; run_inference(task_prompt) {\u0026rsquo;\u0026lt;DETAILED_CAPTION\u0026gt;\u0026rsquo;: \u0026lsquo;In this image we can see two cars on the road. In the background, we can also see water, hills and the sky.\u0026rsquo;}\ntask_prompt = \u0026#39;\u0026lt;MORE_DETAILED_CAPTION\u0026gt;\u0026#39; run_inference(task_prompt) {\u0026rsquo;\u0026lt;MORE_DETAILED_CAPTION\u0026gt;\u0026rsquo;: \u0026lsquo;There are two cars parked on the street. There is water behind the cars. There are mountains behind the water. The cars are yellow and black. \u0026lsquo;}\nObject Detection task_prompt = \u0026#39;\u0026lt;OD\u0026gt;\u0026#39; results = run_inference(task_prompt) print(results) import matplotlib.pyplot as plt import matplotlib.patches as patches def plot_bbox(image, data): # Create a figure and axes fig, ax = plt.subplots() # Display the image ax.imshow(image) # Plot each bounding box for bbox, label in zip(data[\u0026#39;bboxes\u0026#39;], data[\u0026#39;labels\u0026#39;]): # Unpack the bounding box coordinates x1, y1, x2, y2 = bbox # Create a Rectangle patch rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor=\u0026#39;r\u0026#39;, facecolor=\u0026#39;none\u0026#39;) # Add the rectangle to the Axes ax.add_patch(rect) # Annotate the label plt.text(x1, y1, label, color=\u0026#39;white\u0026#39;, fontsize=8, bbox=dict(facecolor=\u0026#39;red\u0026#39;, alpha=0.5)) # Remove the axis ticks and labels ax.axis(\u0026#39;off\u0026#39;) # Show the plot plt.show() plot_bbox(image, results[\u0026#39;\u0026lt;OD\u0026gt;\u0026#39;]) Dense Region Caption task_prompt = \u0026#39;\u0026lt;DENSE_REGION_CAPTION\u0026gt;\u0026#39; results = run_inference(task_prompt) dense_region_res = results print(results) {\u0026rsquo;\u0026lt;DENSE_REGION_CAPTION\u0026gt;\u0026rsquo;: {\u0026lsquo;bboxes\u0026rsquo;: [[334.8450012207031, 115.95000457763672, 599.4450073242188, 248.5500030517578], [18.584999084472656, 117.45000457763672, 304.6050109863281, 236.\u0026gt; 25001525878906], [113.71499633789062, 177.15000915527344, 172.30499267578125, 235.95001220703125], [404.1449890136719, 187.95001220703125, 453.9150085449219, 248.25001525878906], [26.\u0026gt; 774999618530273, 173.85000610351562, 73.3949966430664, 228.15000915527344], [336.1050109863281, 176.25, 380.2049865722656, 235.95001220703125], [244.125, 216.45001220703125, 290.7449951171875, 230.85000610351562], [546.5250244140625, 236.5500030517578, 588.7349853515625, 245.85000610351562], [481.635009765625, 148.35000610351562, 509.3550109863281, 157.65000915527344]], \u0026rsquo;labels\u0026rsquo;: [\u0026lsquo;yellow sports car\u0026rsquo;, \u0026lsquo;sports car\u0026rsquo;, \u0026lsquo;wheel\u0026rsquo;, \u0026lsquo;wheel\u0026rsquo;, \u0026lsquo;wheel\u0026rsquo;, \u0026lsquo;wheel\u0026rsquo;, \u0026lsquo;wheel\u0026rsquo;, \u0026lsquo;wheel\u0026rsquo;, \u0026lsquo;wheel\u0026rsquo;]}}\nplot_bbox(image, results[\u0026#39;\u0026lt;DENSE_REGION_CAPTION\u0026gt;\u0026#39;]) Phrase Grounding task_prompt = \u0026#39;\u0026lt;CAPTION_TO_PHRASE_GROUNDING\u0026gt;\u0026#39; results = run_inference(task_prompt, text_input=\u0026#34;Yellow car with islands in background\u0026#34;) print(results) plot_bbox(image, results[\u0026#39;\u0026lt;CAPTION_TO_PHRASE_GROUNDING\u0026gt;\u0026#39;]) {\u0026rsquo;\u0026lt;CAPTION_TO_PHRASE_GROUNDING\u0026gt;\u0026rsquo;: {\u0026lsquo;bboxes\u0026rsquo;: [[335.4750061035156, 115.6500015258789, 601.9649658203125, 250.35000610351562], [0.3149999976158142, 12.15000057220459, 629.0549926757812, 103.6500015258789]], \u0026rsquo;labels\u0026rsquo;: [\u0026lsquo;Yellow car\u0026rsquo;, \u0026lsquo;islands\u0026rsquo;]}} Segmentation task_prompt = \u0026#39;\u0026lt;REFERRING_EXPRESSION_SEGMENTATION\u0026gt;\u0026#39; results = run_inference(task_prompt, text_input=\u0026#34;yellow car and island\u0026#34;) print(results) from PIL import Image, ImageDraw, ImageFont import random import numpy as np colormap = [\u0026#39;blue\u0026#39;,\u0026#39;orange\u0026#39;,\u0026#39;green\u0026#39;,\u0026#39;purple\u0026#39;,\u0026#39;brown\u0026#39;,\u0026#39;pink\u0026#39;,\u0026#39;gray\u0026#39;,\u0026#39;olive\u0026#39;,\u0026#39;cyan\u0026#39;,\u0026#39;red\u0026#39;, \u0026#39;lime\u0026#39;,\u0026#39;indigo\u0026#39;,\u0026#39;violet\u0026#39;,\u0026#39;aqua\u0026#39;,\u0026#39;magenta\u0026#39;,\u0026#39;coral\u0026#39;,\u0026#39;gold\u0026#39;,\u0026#39;tan\u0026#39;,\u0026#39;skyblue\u0026#39;] def draw_polygons(image, prediction, fill_mask=False): \u0026#34;\u0026#34;\u0026#34; Draws segmentation masks with polygons on an image. Parameters: - image_path: Path to the image file. - prediction: Dictionary containing \u0026#39;polygons\u0026#39; and \u0026#39;labels\u0026#39; keys. \u0026#39;polygons\u0026#39; is a list of lists, each containing vertices of a polygon. \u0026#39;labels\u0026#39; is a list of labels corresponding to each polygon. - fill_mask: Boolean indicating whether to fill the polygons with color. \u0026#34;\u0026#34;\u0026#34; # Load the image draw = ImageDraw.Draw(image) # Set up scale factor if needed (use 1 if not scaling) scale = 1 # Iterate over polygons and labels for polygons, label in zip(prediction[\u0026#39;polygons\u0026#39;], prediction[\u0026#39;labels\u0026#39;]): color = random.choice(colormap) fill_color = random.choice(colormap) if fill_mask else None for _polygon in polygons: _polygon = np.array(_polygon).reshape(-1, 2) if len(_polygon) \u0026lt; 3: print(\u0026#39;Invalid polygon:\u0026#39;, _polygon) continue _polygon = (_polygon * scale).reshape(-1).tolist() # Draw the polygon if fill_mask: draw.polygon(_polygon, outline=color, fill=fill_color) else: draw.polygon(_polygon, outline=color) # Draw the label text draw.text((_polygon[0] + 8, _polygon[1] + 2), label, fill=color) # Save or display the image #image.show() # Display the image display(image) output_image = copy.deepcopy(image) draw_polygons(output_image, results[\u0026#39;\u0026lt;REFERRING_EXPRESSION_SEGMENTATION\u0026gt;\u0026#39;], fill_mask=True) Regions Segmentation def bbox_to_loc(bbox): # bbox position need to be rescaled from 0 to 999. the coordinates are x1_y1_x2_y2 return f\u0026#34;\u0026lt;loc_{int(bbox[0]*999/width)}\u0026gt;\u0026lt;loc_{int(bbox[1]*999/height)}\u0026gt;\u0026lt;loc_{int(bbox[2]*999/width)}\u0026gt;\u0026lt;loc_{int(bbox[3]*999/height)}\u0026gt;\u0026#34; with torch.no_grad(): torch.cuda.empty_cache() output_image = copy.deepcopy(image) height, width = image.height, image.width task_prompt = \u0026#39;\u0026lt;REGION_TO_SEGMENTATION\u0026gt;\u0026#39; for bbox in dense_region_res[\u0026#39;\u0026lt;DENSE_REGION_CAPTION\u0026gt;\u0026#39;][\u0026#39;bboxes\u0026#39;][:]: print(bbox_to_loc(bbox)) results = run_inference(task_prompt, text_input=bbox_to_loc(bbox)) draw_polygons(output_image, results[task_prompt], fill_mask=True) plot_bbox(output_image, dense_region_res[\u0026#39;\u0026lt;DENSE_REGION_CAPTION\u0026gt;\u0026#39;]) \u0026lt;loc_530\u0026gt;\u0026lt;loc_386\u0026gt;\u0026lt;loc_950\u0026gt;\u0026lt;loc_827\u0026gt; \u0026lt;loc_29\u0026gt;\u0026lt;loc_391\u0026gt;\u0026lt;loc_483\u0026gt;\u0026lt;loc_786\u0026gt; \u0026lt;loc_180\u0026gt;\u0026lt;loc_589\u0026gt;\u0026lt;loc_273\u0026gt;\u0026lt;loc_785\u0026gt; \u0026lt;loc_640\u0026gt;\u0026lt;loc_625\u0026gt;\u0026lt;loc_719\u0026gt;\u0026lt;loc_826\u0026gt; \u0026lt;loc_42\u0026gt;\u0026lt;loc_578\u0026gt;\u0026lt;loc_116\u0026gt;\u0026lt;loc_759\u0026gt; \u0026lt;loc_532\u0026gt;\u0026lt;loc_586\u0026gt;\u0026lt;loc_602\u0026gt;\u0026lt;loc_785\u0026gt; \u0026lt;loc_387\u0026gt;\u0026lt;loc_720\u0026gt;\u0026lt;loc_461\u0026gt;\u0026lt;loc_768\u0026gt; \u0026lt;loc_866\u0026gt;\u0026lt;loc_787\u0026gt;\u0026lt;loc_933\u0026gt;\u0026lt;loc_818\u0026gt; \u0026lt;loc_763\u0026gt;\u0026lt;loc_494\u0026gt;\u0026lt;loc_807\u0026gt;\u0026lt;loc_524\u0026gt; OCR url = \u0026#34;https://m.media-amazon.com/images/I/510sf0pRTlL.jpg\u0026#34; image = Image.open(requests.get(url, stream=True).raw).convert(\u0026#39;RGB\u0026#39;) image task_prompt = \u0026#39;\u0026lt;OCR_WITH_REGION\u0026gt;\u0026#39; results = run_inference(task_prompt) print(results) {\u0026rsquo;\u0026lt;OCR_WITH_REGION\u0026gt;\u0026rsquo;: {\u0026lsquo;quad_boxes\u0026rsquo;: [[143.8125, 146.25, 280.9624938964844, 146.25, 280.9624938964844, 172.25, 143.8125, 172.25], [134.0625, 176.25, 281.9375, 176.25, 281.9375, 202.25, 134.0625, 202.25], [172.73748779296875, 206.25, 284.2124938964844, 206.25, 284.2124938964844, 216.25, 172.73748779296875, 216.25], [150.3125, 238.25, 281.9375, 238.25, 281.9375, 247.25, 150.3125, 247.25], [139.58749389648438, 254.25, 284.2124938964844, 254.25, 284.2124938964844, 277.75, 139.58749389648438, 277.75], [133.08749389648438, 283.75, 285.1875, 283.75, 285.1875, 307.75, 133.08749389648438, 307.75], [140.5625, 312.75, 281.9375, 312.75, 281.9375, 320.75, 140.5625, 320.75]], \u0026rsquo;labels\u0026rsquo;: [\u0026rsquo;QUANTUM\u0026rsquo;, \u0026lsquo;MECHANICS\u0026rsquo;, \u0026lsquo;(Non-relativistic Theory)\u0026rsquo;, \u0026lsquo;Course of Theoretical Phyias Volume 3\u0026rsquo;, \u0026lsquo;L.D. LANDAU\u0026rsquo;, \u0026lsquo;E.M. LIFSHITZ\u0026rsquo;, \u0026lsquo;Initiute of Physical Problems, USSR Academy of\u0026rsquo;]}}\nThe overall extracted text from the image is very close to the original one. However, since the image resolution is low, the accuracy on the extracted text is quite low.\ndef draw_ocr_bboxes(image, prediction): scale = 1 draw = ImageDraw.Draw(image) bboxes, labels = prediction[\u0026#39;quad_boxes\u0026#39;], prediction[\u0026#39;labels\u0026#39;] for box, label in zip(bboxes, labels): color = random.choice(colormap) new_box = (np.array(box) * scale).tolist() draw.polygon(new_box, width=3, outline=color) draw.text((new_box[0]-8, new_box[1]-10), \u0026#34;{}\u0026#34;.format(label), align=\u0026#34;right\u0026#34;, fill=color) display(image) output_image = copy.deepcopy(image) draw_ocr_bboxes(output_image, results[\u0026#39;\u0026lt;OCR_WITH_REGION\u0026gt;\u0026#39;]) ","date":"June 25, 2024","hero":"/posts/machine-learning/deep-learning/computer-vision/florence/images/florence-2-lvm-computer-vision-exploration_28_3.png","permalink":"http://localhost:1313/posts/machine-learning/deep-learning/computer-vision/florence/","summary":"Install dependencies Type the following command to install possible needed dependencies (especially if the inference is performed on the CPU)\n%pip install einops flash_attn In Kaggle, transformers and torch are already installed. Otherwise you also need to install them on your local PC.\nImport Libraries from transformers import AutoProcessor, AutoModelForCausalLM from PIL import Image import requests import copy import torch %matplotlib inline Import the model We can choose Florence-2-large or Florence-2-large-ft (fine-tuned).","tags":["Deep Learning","Computer Vision","Machine Learning"],"title":"Florence-2 - Vision Foundation Model - Examples"},{"categories":["Finance"],"contents":"1. Introduction In the dynamic world of finance, options play a crucial role in risk management, speculation, and portfolio optimization. An option is a contract that gives the holder the right, but not the obligation, to buy (call option) or sell (put option) an underlying asset at a predetermined price (strike price) within a specific time frame. The challenge lies in accurately pricing these financial instruments, given the uncertainties in market movements.\nTraditional analytical methods, while powerful, often struggle with complex option structures or realistic market conditions. This is where Monte Carlo simulation steps in, offering a flexible and robust approach to option pricing. By leveraging the power of computational methods, Monte Carlo simulations can handle a wide array of option types and market scenarios, making it an indispensable tool in a quantitative analyst\u0026rsquo;s toolkit.\nFor further explanation about options pricing, check Investopedia.\n2. The Black-Scholes Model Before diving into Monte Carlo methods, it\u0026rsquo;s crucial to understand the Black-Scholes model, a cornerstone in option pricing theory. Developed by Fischer Black, Myron Scholes, and Robert Merton in the early 1970s, this model revolutionized the field of quantitative finance.\nThe Black-Scholes Formula For a European call option, the Black-Scholes formula is:\n$$ C = S₀N(d_1) - Ke^{-rT}N(d_2) $$\nWhere:\n$$ d_1 = \\frac{(ln(S_0/K) + (r + σ²/2)T)}{(σ\\sqrt{T})}, \\quad d_2 = d_1 - \\sigma \\sqrt{T} $$\nC: Call option price S₀: Current stock price K: Strike price r: Risk-free interest rate T: Time to maturity σ: Volatility of the underlying asset N(x): Cumulative standard normal distribution function Assumptions of the Black-Scholes Model The Black-Scholes model rests on several key assumptions:\nThe stock price follows a geometric Brownian motion with constant drift and volatility. No arbitrage opportunities exist in the market. It\u0026rsquo;s possible to buy and sell any amount of stock or options (including fractional amounts). There are no transaction costs or taxes. All securities are perfectly divisible. The risk-free interest rate is constant and known. The underlying stock does not pay dividends. Limitations of the Black-Scholes Model While groundbreaking, the Black-Scholes model has several limitations:\nConstant Volatility: The model assumes volatility is constant, which doesn\u0026rsquo;t hold in real markets where volatility can change dramatically. Log-normal Distribution: It assumes stock returns are normally distributed, which doesn\u0026rsquo;t account for the fat-tailed distributions observed in reality. Continuous Trading: The model assumes continuous trading is possible, which isn\u0026rsquo;t realistic in practice. No Dividends: It doesn\u0026rsquo;t account for dividends, which can significantly affect option prices. European Options Only: The original model only prices European-style options, not American or exotic options. Risk-free Rate: It assumes a constant, known risk-free rate, which can vary in reality. These limitations highlight why more flexible approaches like Monte Carlo simulation are valuable in option pricing.\n3. Monte Carlo Simulation: Theoretical Background Monte Carlo simulation addresses many of the Black-Scholes model\u0026rsquo;s limitations by using computational power to model a wide range of possible future scenarios.\nBasic Principles Monte Carlo methods use repeated random sampling to obtain numerical results. In the context of option pricing, we simulate many possible price paths for the underlying asset and then calculate the option\u0026rsquo;s payoff for each path.\nApplication to Option Pricing For option pricing, we model the stock price movement using a stochastic differential equation:\n$$ dS = \\mu Sdt + \\sigma SdW $$\nWhere:\nS: Stock price μ: Expected return σ: Volatility dW: Wiener process (random walk) This equation is then discretized for simulation purposes.\n4. Implementing Monte Carlo Simulation in Python Let\u0026rsquo;s implement a basic Monte Carlo simulation for pricing a European call option:\nimport numpy as np import matplotlib.pyplot as plt def monte_carlo_option_pricing(S0, K, T, r, sigma, num_simulations, num_steps): dt = T / num_steps paths = np.zeros((num_simulations, num_steps + 1)) paths[:, 0] = S0 for t in range(1, num_steps + 1): z = np.random.standard_normal(num_simulations) paths[:, t] = paths[:, t-1] * np.exp((r - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * z) option_payoffs = np.maximum(paths[:, -1] - K, 0) option_price = np.exp(-r * T) * np.mean(option_payoffs) return option_price, paths # Example usage S0 = 100 # Initial stock price K = 98.5 # Strike price T = 1 # Time to maturity (in years) r = 0.05 # Risk-free rate sigma = 0.2 # Volatility num_simulations = 10000 num_steps = 252 # Number of trading days in a year price, paths = monte_carlo_option_pricing(S0, K, T, r, sigma, num_simulations, num_steps) print(f\u0026#34;Estimated option price: {price:.2f}\u0026#34;) This code simulates multiple stock price paths, calculates the option payoff for each path, and then averages these payoffs to estimate the option price.\n5. Visualization and Analysis Visualizing the results helps in understanding the distribution of possible outcomes:\nplt.figure(figsize=(10, 6)) plt.plot(paths[:100, :].T) plt.title(\u0026#34;Sample Stock Price Paths\u0026#34;) plt.xlabel(\u0026#34;Time Steps\u0026#34;) plt.ylabel(\u0026#34;Stock Price\u0026#34;) plt.show() plt.figure(figsize=(10, 6)) plt.hist(paths[:, -1], bins=50) plt.title(\u0026#34;Distribution of Final Stock Prices\u0026#34;) plt.xlabel(\u0026#34;Stock Price\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.show() These visualizations show the range of possible stock price paths and the distribution of final stock prices, providing insight into the option\u0026rsquo;s potential outcomes.\n6. Comparison with Analytical Solutions To validate our Monte Carlo results, we can compare them with the Black-Scholes analytical solution:\nfrom scipy.stats import norm def black_scholes_call(S0, K, T, r, sigma): d1 = (np.log(S0 / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T)) d2 = d1 - sigma * np.sqrt(T) return S0 * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2) bs_price = black_scholes_call(S0, K, T, r, sigma) print(f\u0026#34;Black-Scholes price: {bs_price:.3f}\u0026#34;) print(f\u0026#34;Monte Carlo price: {price:.3f}\u0026#34;) print(f\u0026#34;Difference: {abs(bs_price - price):.4f}\u0026#34;) The difference between the two methods gives us an idea of the Monte Carlo simulation\u0026rsquo;s accuracy.\nBlack-Scholes price: 11.270\nMonte Carlo price: 11.445\nDifference: 0.1744\n7. Advanced Topics and Extensions Monte Carlo simulation\u0026rsquo;s flexibility allows for various extensions:\nVariance Reduction Techniques: Methods like antithetic variates can improve accuracy without increasing computational cost. Exotic Options: Monte Carlo can price complex options like Asian or barrier options, which are challenging for analytical methods. Incorporating Dividends: We can easily modify the simulation to account for dividend payments. Stochastic Volatility: Models like Heston can be implemented to account for changing volatility. 8. Conclusion Monte Carlo simulation offers a powerful and flexible approach to option pricing, addressing many limitations of analytical methods like the Black-Scholes model. While it can be computationally intensive, it handles complex scenarios and option structures with relative ease.\nThe method\u0026rsquo;s ability to incorporate various market dynamics, such as changing volatility or dividend payments, makes it invaluable in real-world financial modeling. As computational power continues to increase, Monte Carlo methods are likely to play an even more significant role in quantitative finance.\nHowever, it\u0026rsquo;s important to remember that any model, including Monte Carlo simulation, is only as good as its underlying assumptions. Careful consideration of these assumptions and regular validation against market data remain crucial in applying these techniques effectively in practice.\n","date":"June 23, 2024","hero":"/posts/finance/monte_carlo/black-scholes/Option-Pricing-Models-1.jpg","permalink":"http://localhost:1313/posts/finance/monte_carlo/black-scholes/","summary":"1. Introduction In the dynamic world of finance, options play a crucial role in risk management, speculation, and portfolio optimization. An option is a contract that gives the holder the right, but not the obligation, to buy (call option) or sell (put option) an underlying asset at a predetermined price (strike price) within a specific time frame. The challenge lies in accurately pricing these financial instruments, given the uncertainties in market movements.","tags":["Finance","Options","Statistics"],"title":"Monte Carlo Simulation for Option Pricing"},{"categories":["Finance"],"contents":"Introduction In this article, we will explore time series data extracted from the stock market, focusing on prominent technology companies such as Apple, Amazon, Google, and Microsoft. Our objective is to equip data analysts and scientists with the essential skills to effectively manipulate and interpret stock market data.\nTo achieve this, we will utilize the yfinance library to fetch stock information and leverage visualization tools such as Seaborn and Matplotlib to illustrate various facets of the data. Specifically, we will explore methods to analyze stock risk based on historical performance, and implement predictive modeling using GRU/ LSTM models.\nThroughout this tutorial, we aim to address the following key questions:\nHow has the stock price evolved over time? What is the average daily return of the stock? How does the moving average of the stocks vary? What is the correlation between different stocks? How can we forecast future stock behavior, exemplified by predicting the closing price of Apple Inc. using LSTM or GRU?\u0026quot; Getting Data The initial step involves acquiring and loading the data into memory. Our source of stock data is the Yahoo Finance website, renowned for its wealth of financial market data and investment tools. To access this data, we\u0026rsquo;ll employ the yfinance library, known for its efficient and Pythonic approach to downloading market data from Yahoo. For further insights into yfinance, refer to the article titled Reliably download historical market data from with Python.\nInstall Dependencies pip install -qU yfinance seaborn Configuration Code import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns.set_style(\u0026#39;whitegrid\u0026#39;) plt.style.use(\u0026#34;fivethirtyeight\u0026#34;) %matplotlib inline #comment if you are not using a jupyter notebook # For reading stock data from yahoo from pandas_datareader.data import DataReader import yfinance as yf from pandas_datareader import data as pdr yf.pdr_override() # For time stamps from datetime import datetime # Get Microsoft data data = yf.download(\u0026#34;MSFT\u0026#34;, start, end) Statistical Analysis on the price Summary # Summary Stats data.describe() Closing Price The closing price is the last price at which the stock is traded during the regular trading day. A stock’s closing price is the standard benchmark used by investors to track its performance over time.\nplt.figure(figsize=(14, 5)) plt.plot(data[\u0026#39;Adj Close\u0026#39;], label=\u0026#39;Close Price\u0026#39;) plt.xlabel(\u0026#39;Date\u0026#39;) plt.ylabel(\u0026#39;Close Price [$]\u0026#39;) plt.title(\u0026#39;Stock Price History\u0026#39;) plt.legend() plt.show() Volume of Sales Volume is the amount of an asset or security that changes hands over some period of time, often over the course of a day. For instance, the stock trading volume would refer to the number of shares of security traded between its daily open and close. Trading volume, and changes to volume over the course of time, are important inputs for technical traders.\nplt.figure(figsize=(14, 5)) plt.plot(data[\u0026#39;Volume\u0026#39;], label=\u0026#39;Volume\u0026#39;) plt.xlabel(\u0026#39;Date\u0026#39;) plt.ylabel(\u0026#39;Volume\u0026#39;) plt.title(\u0026#39;Stock Price History\u0026#39;) plt.show() Moving Average The moving average (MA) is a simple technical analysis tool that smooths out price data by creating a constantly updated average price. The average is taken over a specific period of time, like 10 days, 20 minutes, 30 weeks, or any time period the trader chooses.\nma_day = [10, 20, 50] # compute moving average (can be also done in a vectorized way) for ma in ma_day: column_name = f\u0026#34;{ma} days MA\u0026#34; data[column_name] = data[\u0026#39;Adj Close\u0026#39;].rolling(ma).mean() plt.figure(figsize=(14, 5)) data[[\u0026#39;Adj Close\u0026#39;, \u0026#39;10 days MA\u0026#39;, \u0026#39;20 days MA\u0026#39;, \u0026#39;50 days MA\u0026#39;]].plot() plt.xlabel(\u0026#39;Date\u0026#39;) plt.ylabel(\u0026#39;Volume\u0026#39;) plt.title(\u0026#39;Stock Price History\u0026#39;) plt.show() Statistical Analysis on the returns Now that we\u0026rsquo;ve done some baseline analysis, let\u0026rsquo;s go ahead and dive a little deeper. We\u0026rsquo;re now going to analyze the risk of the stock. In order to do so we\u0026rsquo;ll need to take a closer look at the daily changes of the stock, and not just its absolute value. Let\u0026rsquo;s go ahead and use pandas to retrieve teh daily returns for the Microsoft stock.\n# Compute daily return in percentage data[\u0026#39;Daily Return\u0026#39;] = data[\u0026#39;Adj Close\u0026#39;].pct_change() # simple plot plt.figure(figsize=(14, 5)) data[\u0026#39;Daily Return\u0026#39;].hist(bins=50) plt.title(\u0026#39;MSFT Daily Return Distribution\u0026#39;) plt.xlabel(\u0026#39;Daily Return\u0026#39;) plt.show() # histogram plt.figure(figsize=(8, 5)) data[\u0026#39;Daily Return\u0026#39;].plot() plt.title(\u0026#39;MSFT Daily Return\u0026#39;) plt.show() Data Preparation # Create a new dataframe with only the \u0026#39;Close column X = data.filter([\u0026#39;Adj Close\u0026#39;]) # Convert the dataframe to a numpy array X = X.values # Get the number of rows to train the model on training_data_len = int(np.ceil(len(X)*.95)) # Scale data from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler(feature_range=(0,1)) scaled_data = scaler.fit_transform(X) scaled_data Split training data into small chunks to ingest into LSTM and GRU\n# Create the training data set # Create the scaled training data set train_data = scaled_data[0:int(training_data_len), :] # Split the data into x_train and y_train data sets x_train = [] y_train = [] seq_length = 60 for i in range(seq_length, len(train_data)): x_train.append(train_data[i-60:i, 0]) y_train.append(train_data[i, 0]) if i\u0026lt;= seq_length+1: print(x_train) print(y_train, end=\u0026#34;\\n\\n\u0026#34;) # Convert the x_train and y_train to numpy arrays x_train, y_train = np.array(x_train), np.array(y_train) # Reshape the data x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1)) GRU Gated-Recurrent Unit (GRU) is adopted in this part\nfrom tensorflow.keras.models import Sequential from tensorflow.keras.layers import GRU, Dense, Dropout lstm_model = Sequential() lstm_model.add(GRU(units=128, return_sequences=True, input_shape=(seq_length, 1))) lstm_model.add(Dropout(0.2)) lstm_model.add(GRU(units=64, return_sequences=False)) lstm_model.add(Dropout(0.2)) lstm_model.add(Dense(units=1)) lstm_model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;mean_squared_error\u0026#39;) lstm_model.fit(x_train, y_train, epochs=10, batch_size=8) LSTM Long Short-Term Memory (LSTM) is adopted in this part\nfrom tensorflow.keras.layers import LSTM lstm_model = Sequential() lstm_model.add(LSTM(units=128, return_sequences=True, input_shape=(seq_length, 1))) lstm_model.add(Dropout(0.2)) lstm_model.add(LSTM(units=64, return_sequences=False)) lstm_model.add(Dropout(0.2)) lstm_model.add(Dense(units=1)) lstm_model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;mean_squared_error\u0026#39;) lstm_model.fit(x_train, y_train, epochs=10, batch_size=8) Testing Metrics root mean squared error (RMSE) # Create the testing data set # Create a new array containing scaled values from index 1543 to 2002 test_data = scaled_data[training_data_len - 60: , :] # Create the data sets x_test and y_test x_test = [] y_test = dataset[training_data_len:, :] for i in range(60, len(test_data)): x_test.append(test_data[i-60:i, 0]) # Convert the data to a numpy array x_test = np.array(x_test) # Reshape the data x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 )) # Get the models predicted price values predictions_gru = gru_model.predict(x_test) predictions_gru = scaler.inverse_transform(predictions_gru) predictions_lstm = lstm_model.predict(x_test) predictions_lstm = scaler.inverse_transform(predictions_lstm) # Get the root mean squared error (RMSE) rmse_lstm = np.sqrt(np.mean(((predictions_lstm - y_test) ** 2))) rmse_gru = np.sqrt(np.mean(((predictions_gru - y_test) ** 2))) print(f\u0026#34;LSTM RMSE: {rmse_lstm:.4f}, GRU RMSE: {rmse_gru:.4f}\u0026#34;) \u0026ldquo;LSTM RMSE: 4.2341, GRU RMSE: 3.3575\u0026rdquo;\nTest Plot GRU-based model shows a bit better results both graphically and on MSE. However, this does not tell us anything about the actual profitability of these models.\nPossible trading performance The strategy implementation is:\nBUY: if prediction \u0026gt; actual_price SELL: if prediction \u0026lt; actual_price To close a position the next candle close is waited. However, LSTM and GRU has some offset that does not allow a proper utilization of this strategy.\nHence, the returns of the predictions are adopted.\n# Assume a trading capital of $10,000 trading_capital = 10000 pred_gru_df = pd.DataFrame(predictions_gru, columns=[\u0026#39;Price\u0026#39;]) pred_test_df = pd.DataFrame(y_test, columns=[\u0026#39;Price\u0026#39;]) pred_gru_df[\u0026#39;returns\u0026#39;] = pred_gru_df.pct_change(-1) pred_test_df[\u0026#39;returns\u0026#39;] = pred_test_df.pct_change(-1) # Compute Wins wins = ((pred_gru_df.dropna().returns\u0026lt;0) \u0026amp; (pred_test_df.dropna().returns\u0026lt;0)) | ((pred_gru_df.dropna().returns\u0026gt;0) \u0026amp; (pred_test_df.dropna().returns\u0026gt;0)) print(wins.value_counts()) returns_df = pd.concat([pred_gru_df.returns, pred_test_df.returns], axis=1).dropna() total_pos_return = pred_test_df.dropna().returns[wins].abs().sum() total_neg_return = pred_test_df.dropna().returns[np.logical_not(wins)].abs().sum() # compute final capital and compare with BUY\u0026amp;HOLD strategy final_capital = trading_capital*(1+total_pos_return-total_neg_return) benchmark_return = (valid.Close.iloc[-1] - valid.Close.iloc[0])/valid.Close.iloc[0] bench_capital = trading_capital*(1+benchmark_return) print(final_capital, bench_capital) returns\nTrue 81 False 72\nName: count, dtype: int64\n10535.325 9617.617\nConclusion As showed in the previous section, these two simple Deep Learning models exhibits interesting positive results both regarding regression and trading metrics. The latter is particularly important, indeed a return of 5% is obtained while the stock price decreased of approximately 4%. This also lead to a very high sharpe and colmar ratio.\n","date":"June 16, 2024","hero":"/posts/finance/stock_prediction/gru/images/stock-market-prediction-using-data-mining-techniques.jpg","permalink":"http://localhost:1313/posts/finance/stock_prediction/gru/","summary":"Introduction In this article, we will explore time series data extracted from the stock market, focusing on prominent technology companies such as Apple, Amazon, Google, and Microsoft. Our objective is to equip data analysts and scientists with the essential skills to effectively manipulate and interpret stock market data.\nTo achieve this, we will utilize the yfinance library to fetch stock information and leverage visualization tools such as Seaborn and Matplotlib to illustrate various facets of the data.","tags":["Finance","Deep Learning","Forecasting"],"title":"MSFT Stock Prediction using LSTM or GRU"},{"categories":["Physics"],"contents":"Introduction Percolation theory is a fundamental concept in statistical physics and mathematics that describes the behavior of connected clusters in a random graph. It is a model for understanding how a network behaves when nodes or links are added, leading to a phase transition from a state of disconnected clusters to a state where a large, connected cluster spans the system. This transition occurs at a critical threshold, known as the percolation threshold. The theory has applications in various fields, including material science, epidemiology, and network theory.\nWhy is Percolation Important? Useful Applications Percolation theory is important because it provides insights into the behavior of complex systems and phase transitions. Here are some key applications:\nMaterial Science: Percolation theory helps in understanding the properties of composite materials, such as conductivity and strength. For example, the electrical conductivity of a composite material can change dramatically when the concentration of conductive filler reaches the percolation threshold Epidemiology: In the study of disease spread, percolation models can predict the outbreak and spread of epidemics. The percolation threshold can represent the critical point at which a disease becomes widespread in a population Network Theory: Percolation theory is used to study the robustness and connectivity of networks, such as the internet or social networks. It helps in understanding how networks can be disrupted and how they can be made more resilient Geophysics: In oil recovery, percolation theory models the flow of fluids through porous rocks, helping to optimize extraction processes Forest Fires: Percolation models can simulate the spread of forest fires, helping in the development of strategies for fire prevention and control Mathematical and Physics Theory Percolation theory can be studied using site percolation or bond percolation models. In site percolation, each site (or node) on a lattice is either occupied with probability $ p $ or empty with probability $ 1 - p $. In bond percolation, each bond (or edge) between sites is open with probability $ p $ or closed with probability $ 1 - p $.\nStep-by-Step Explanation: Define the Lattice: Consider a 2D square lattice or a 3D cubic lattice. For simplicity, let\u0026rsquo;s use a 2D square lattice.\nAssign Probabilities: For each site (or bond), assign a probability $ p $ that it is occupied (or open).\nCluster Formation: Identify clusters of connected sites (or bonds). Two sites are in the same cluster if there is a path of occupied sites (or open bonds) connecting them.\nCritical Threshold $ p_c $: Determine the critical probability $ p_c $ at which an infinite cluster first appears. For a 2D square lattice, it has been rigorously shown that $ p_c \\approx 0.5927 $.\nMathematical Formulation: The percolation probability $ P(p) $ is the probability that a given site belongs to the infinite cluster. Near the critical threshold, this follows a power-law behavior: $$ P(p) \\sim (p - p_c)^\\beta $$ where $ \\beta $ is a critical exponent and equal to $\\frac{5}{36}$ for 2D squared lattice.\nCorrelation Length $ \\xi $: The average size of finite clusters below $ p_c $ is characterized by the correlation length $ \\xi $, which diverges as: $$ \\xi \\sim |p - p_c|^{-\\nu} $$ where $ \\nu $ is another critical exponent\nConductivity and Other Properties: In practical applications, properties like electrical conductivity in materials can be modeled by considering the effective medium theory or numerical simulations to calculate the likelihood of percolation and the size of clusters.\nBy analyzing these steps, percolation theory provides a comprehensive understanding of how macroscopic properties emerge from microscopic randomness, revealing universal behaviors that transcend specific systems.\nPython Simulation Code Here is a simple example of a site percolation simulation on a square lattice in Python:\n# -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; Created on Wed Jun 19 07:15:50 2024 @author: stefa \u0026#34;\u0026#34;\u0026#34; import numpy as np import matplotlib.pyplot as plt import scipy.ndimage def generate_lattice(n, p): \u0026#34;\u0026#34;\u0026#34;Generate an n x n lattice with site vacancy probability p.\u0026#34;\u0026#34;\u0026#34; return np.random.rand(n, n) \u0026lt; p def percolates(lattice): \u0026#34;\u0026#34;\u0026#34;Check if the lattice percolates.\u0026#34;\u0026#34;\u0026#34; intersection = get_intersection(lattice) return intersection.size \u0026gt; 1 def get_intersection(lattice): \u0026#34;\u0026#34;\u0026#34;Check if the bottom labels or top labels have more than 1 value equal \u0026#34;\u0026#34;\u0026#34; labeled_lattice, num_features = scipy.ndimage.label(lattice) top_labels = np.unique(labeled_lattice[0, :]) bottom_labels = np.unique(labeled_lattice[-1, :]) print(top_labels, bottom_labels, np.intersect1d(top_labels, bottom_labels).size) return np.intersect1d(top_labels, bottom_labels) def plot_lattice(lattice): \u0026#34;\u0026#34;\u0026#34;Plot the lattice.\u0026#34;\u0026#34;\u0026#34; plt.imshow(lattice, cmap=\u0026#39;binary\u0026#39;) plt.show() def lattice_to_image(lattice): \u0026#34;\u0026#34;\u0026#34;Convert the lattice in an RGB image (even if will be black and white)\u0026#34;\u0026#34;\u0026#34; r_ch = np.where(lattice, 0, 255) g_ch = np.where(lattice, 0, 255) b_ch = np.where(lattice, 0, 255) image = np.concatenate(( np.expand_dims(r_ch, axis=2), np.expand_dims(g_ch, axis=2), np.expand_dims(b_ch, axis=2), ), axis=2) return image def get_path(lattice): intersection = get_intersection(lattice) labeled_lattice, num_features = scipy.ndimage.label(lattice) print(intersection) return labeled_lattice == intersection[1] def add_path_img(image, path): blank_image = np.zeros(image.shape) # set the red channel to 255-\u0026gt; get a red percolation path image[path, 0] = 255 image[path, 1] = 20 return image # Parameters n = 100 # Lattice size p_values = [0.2, 0.3, 0.4, 0.5, 0.58, 0.6] # Site vacancy probabilities # Create a figure with subplots fig, axs = plt.subplots(2, 3, figsize=(15, 8)) axs = axs.flatten() # Plot lattices for different p values for i, p in enumerate(p_values): lattice = generate_lattice(n, p) lattice_img = lattice_to_image(lattice) if percolates(lattice): axs[i].set_title(f\u0026#34;p = {p} (Percolates)\u0026#34;) path = get_path(lattice) lattice_img = add_path_img(lattice_img, path) else: axs[i].set_title(f\u0026#34;p = {p} (Does not percolate)\u0026#34;) axs[i].imshow(lattice_img) axs[i].axis(\u0026#39;off\u0026#39;) # Adjust spacing between subplots plt.subplots_adjust(wspace=0.1, hspace=0.1) # Show the plot plt.show() This code generates a square lattice of size n with site vacancy probability p, checks if the lattice percolates (i.e., if there is a connected path from the top to the bottom), and plots the lattice.\nIn further version, also a connected path from left to right can be considered.\nResults Conclusion The previous plot shows that with p\u0026gt;0.58 a percolation path starts to be observed. However, this is so not alwasy happening for stochastical reasons. Hence that plot is the result of several iteration to find the most interesting plot. With p\u0026gt;0.60 percolation happens more than 90% of the time. In general, this confirms the numerical value of $p_c$ that can be found in literature of 0.5927\nIn further articles we will explore some python libraries to develop a more advanced and practical example.\n","date":"June 8, 2024","hero":"/posts/physics/percolation/images/lattice_illustration.png","permalink":"http://localhost:1313/posts/physics/percolation/","summary":"Introduction Percolation theory is a fundamental concept in statistical physics and mathematics that describes the behavior of connected clusters in a random graph. It is a model for understanding how a network behaves when nodes or links are added, leading to a phase transition from a state of disconnected clusters to a state where a large, connected cluster spans the system. This transition occurs at a critical threshold, known as the percolation threshold.","tags":["Science","Physics","Statistics"],"title":"Percolation"}]